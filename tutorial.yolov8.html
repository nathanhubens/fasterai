<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="YOLOV8">

<title>YOLOV8 – fasterai</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-424WWZFZ5F"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-424WWZFZ5F', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="YOLOV8 – fasterai">
<meta property="og:description" content="YOLOV8">
<meta property="og:site_name" content="fasterai">
<meta name="twitter:title" content="YOLOV8 – fasterai">
<meta name="twitter:description" content="YOLOV8">
<meta name="twitter:creator" content="@nathanhubens">
<meta name="twitter:site" content="@fasterai">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">fasterai</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://nathanhubens.github.io/fasterai/quickstart.html"> 
<span class="menu-text">Get Started</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-help" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Help</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-help">    
        <li>
    <a class="dropdown-item" href="https://github.com/fastai/nbdev/issues"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an Issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="mailto:nathan.hubens@gmail.com?subject=Hello"><i class="bi bi-chat-right-text" role="img">
</i> 
 <span class="dropdown-text">Contact Me</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://discord.gg/32BwhJSB9u"><i class="bi bi-discord" role="img">
</i> 
 <span class="dropdown-text">Join the Community</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/nathanhubens/fasterai"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/fasterai"> <i class="bi bi-twitter" role="img" aria-label="FasterAI Twitter">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tutorial.walkthrough.html">Tutorials</a></li><li class="breadcrumb-item"><a href="./tutorial.prune_callback.html">Prune</a></li><li class="breadcrumb-item"><a href="./tutorial.yolov8.html">YOLOV8</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quickstart.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick Start</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Tutorials</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.walkthrough.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Walkthrough</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Sparse</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.schedules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedules</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.sparsifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.sparsify_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsify Callback</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.lottery_ticket.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lottery Ticket Hypothesis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prune Transformers</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Prune</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.prune_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prune Callback</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.yolov8.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">YOLOV8</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Distill</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.knowledge_distillation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KnowledgeDistillation Callback</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Regularize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.regularizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regularize Callback</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.bn_folding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BatchNorm Folding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorial.fc_decomposer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fully-Connected layers decomposition</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Core</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.granularity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Granularity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.criteria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Criteria</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.schedules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedules</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Sparse</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sparse.sparsifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sparse.sparsify_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsify Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Prune</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prune.pruner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Pruner</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prune.prune_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prune Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Distill</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./distill.knowledge_distillation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knowledge Distillation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Quantize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quantize.quantizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantizer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quantize.quantize_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantize Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Regularize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regularize.regularizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regularize Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./misc.bn_folding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Norm Folding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./misc.fc_decomposer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fully-Connected Layers Decomposer</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#helpers" id="toc-helpers" class="nav-link active" data-scroll-target="#helpers">Helpers</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#post-training-checks" id="toc-post-training-checks" class="nav-link" data-scroll-target="#post-training-checks">Post-Training Checks</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/nathanhubens/fasterai/tree/master/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tutorial.walkthrough.html">Tutorials</a></li><li class="breadcrumb-item"><a href="./tutorial.prune_callback.html">Prune</a></li><li class="breadcrumb-item"><a href="./tutorial.yolov8.html">YOLOV8</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">YOLOV8</h1>
</div>

<div>
  <div class="description">
    YOLOV8
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="helpers" class="level2">
<h2 class="anchored" data-anchor-id="helpers">Helpers</h2>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<div id="30b08319-6d07-4e42-9528-3e09a0558e93" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Args(argparse.Namespace):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> <span class="st">'yolov8l.pt'</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  cfg <span class="op">=</span> <span class="st">'default.yaml'</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  iterative_steps <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  target_prune_rate <span class="op">=</span> <span class="fl">0.15</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  max_map_drop <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  sched <span class="op">=</span> Schedule(partial(sched_onecycle,  α<span class="op">=</span><span class="dv">10</span>, β<span class="op">=</span><span class="dv">4</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>args<span class="op">=</span>Args()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>prune(args)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.731      0.768      0.828      0.659
Speed: 0.1ms preprocess, 7.7ms inference, 0.0ms loss, 0.6ms postprocess per image
Results saved to runs/detect/val59
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train49
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Before Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.65869</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/train49/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/train49
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      14.4G     0.8537     0.7447      1.082        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.776      0.741      0.832      0.667

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      12.8G     0.8612     0.7059      1.079        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.859       0.75      0.861      0.697

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      12.7G     0.8249     0.6306      1.054        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.882      0.753      0.862      0.709

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      12.8G     0.7998     0.5746      1.047         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.882      0.799       0.87      0.721

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      13.1G     0.8028     0.5566      1.034         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.88      0.804      0.876      0.728

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      12.8G     0.8042     0.5415      1.047        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.896      0.833      0.901      0.741

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      12.8G     0.7493     0.5095      1.003         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.906      0.827      0.902      0.746

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      12.8G     0.7589     0.5373      1.012        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.91      0.828      0.903      0.749

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      12.8G     0.7234     0.4783     0.9947        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.912      0.832      0.906      0.754

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      12.7G     0.7445     0.4764     0.9944        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.905      0.839      0.905      0.754

10 epochs completed in 0.027 hours.
Optimizer stripped from runs/detect/train49/weights/last.pt, 175.3MB
Optimizer stripped from runs/detect/train49/weights/best.pt, 175.3MB

Validating runs/detect/train49/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.904      0.841      0.905      0.755
Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.3ms postprocess per image
Results saved to runs/detect/train49
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.917      0.823      0.901      0.754
Speed: 0.2ms preprocess, 10.8ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/baseline_val184</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Before Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.75438
Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.27046189978777607
After Pruning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 43325836 parameters, 74176 gradients, 163.3 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.883      0.849      0.903      0.743
Speed: 0.2ms preprocess, 12.4ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_0_pre_val131
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_0_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_0_finetune103
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 1: MACs=81.8125668 G, #Params=43.348966 M, mAP=0.7428735001565969, speed up=1.0111699172357467</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_0_finetune103/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_0_finetune103
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.6G     0.7161     0.4777     0.9953        122        640: 100%|██████████| 8/8 [00:03
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.92      0.841      0.907       0.75

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      13.3G     0.6503     0.4152     0.9541        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.904      0.851       0.91      0.765

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      13.2G     0.6809      0.434     0.9746        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.902      0.854      0.907      0.768

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      13.2G     0.6464     0.4095     0.9681         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.919      0.853      0.913      0.773

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      13.2G     0.6799     0.4357     0.9637         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.916      0.856      0.918      0.779

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      13.2G     0.6787     0.4261     0.9708        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.924      0.851      0.923       0.78

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      13.2G     0.6758     0.4286      0.957         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.91      0.852       0.92      0.785

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      13.2G     0.6818     0.4492     0.9705        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.899      0.856      0.921      0.786

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      13.2G     0.6594      0.429     0.9635        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.894      0.871      0.925      0.794

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      13.2G     0.6705     0.4277     0.9541        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.899       0.87      0.927      0.793

10 epochs completed in 0.038 hours.
Optimizer stripped from runs/detect/step_0_finetune103/weights/last.pt, 173.9MB
Optimizer stripped from runs/detect/step_0_finetune103/weights/best.pt, 173.9MB

Validating runs/detect/step_0_finetune103/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 43325836 parameters, 0 gradients, 163.3 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.895       0.87      0.925      0.793
Speed: 0.1ms preprocess, 5.1ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_0_finetune103
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 43325836 parameters, 0 gradients, 163.3 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.897      0.863      0.922      0.787
Speed: 0.2ms preprocess, 12.4ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_0_post_val75
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.7869655326484724
After post fine-tuning validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
0.5179586515491672
After Pruning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 43081939 parameters, 74176 gradients, 162.7 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.913      0.872      0.929      0.788
Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_1_pre_val66
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_1_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_1_finetune62
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 2: MACs=81.5020432 G, #Params=43.105009 M, mAP=0.7879549975477981, speed up=1.0150224847369225</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_1_finetune62/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_1_finetune62
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.3G     0.5906     0.3832     0.9224        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.903      0.868      0.925      0.797

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      13.3G     0.5313     0.3408     0.9001        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.908       0.87      0.925      0.796

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      13.2G     0.5791     0.3608     0.9246        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.902      0.871      0.927      0.799

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      13.3G     0.5612     0.3602     0.9298         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.914      0.867      0.929      0.795

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      13.2G     0.5719     0.3787     0.9132         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.906      0.875       0.93      0.792

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      13.2G     0.5878     0.3844     0.9333        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.935      0.859       0.93      0.796

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      13.3G     0.5939     0.3776     0.9134         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.928       0.86      0.928      0.799

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      13.2G     0.6093     0.3903     0.9311        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.862      0.933      0.802

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      13.3G     0.6003      0.386     0.9318        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.92      0.877      0.935      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      13.2G     0.6393      0.408     0.9322        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.911      0.888      0.937      0.806

10 epochs completed in 0.025 hours.
Optimizer stripped from runs/detect/step_1_finetune62/weights/last.pt, 173.0MB
Optimizer stripped from runs/detect/step_1_finetune62/weights/best.pt, 173.0MB

Validating runs/detect/step_1_finetune62/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.911      0.888      0.937      0.806
Speed: 0.1ms preprocess, 5.1ms inference, 0.0ms loss, 0.3ms postprocess per image
Results saved to runs/detect/step_1_finetune62
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.907      0.888      0.937      0.804
Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_1_post_val48
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.804165683147925
After post fine-tuning validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
0.9769531739708688
After Pruning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 42712366 parameters, 74176 gradients, 161.3 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.923      0.871      0.933      0.794
Speed: 0.2ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_2_pre_val50
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_2_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_2_finetune48
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 3: MACs=80.7933916 G, #Params=42.735334 M, mAP=0.7940590327289188, speed up=1.0239254072854147</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_2_finetune48/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_2_finetune48
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.4G      0.548     0.3528     0.9023        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.923      0.871      0.935      0.801

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      13.3G     0.4746     0.3015     0.8763        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.93      0.877       0.94      0.806

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      13.2G     0.5379     0.3445     0.9065        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.871      0.942      0.812

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      13.3G     0.5157     0.3339     0.9019         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.93      0.877      0.938      0.811

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      13.3G     0.5169     0.3404     0.8804         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.921      0.883      0.939       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      13.3G     0.5339     0.3559     0.9031        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.929      0.878       0.94      0.811

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      13.3G     0.5597     0.3561     0.8945         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.876      0.941      0.813

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      13.3G     0.5733     0.3857     0.9149        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.936      0.879      0.941      0.818

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      13.3G     0.5769     0.3717     0.9222        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.941      0.882      0.941      0.821

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      13.2G     0.6167     0.3871     0.9151        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.882      0.941      0.821

10 epochs completed in 0.024 hours.
Optimizer stripped from runs/detect/step_2_finetune48/weights/last.pt, 171.5MB
Optimizer stripped from runs/detect/step_2_finetune48/weights/best.pt, 171.5MB

Validating runs/detect/step_2_finetune48/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.882      0.942      0.822
Speed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_2_finetune48
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.92      0.888      0.943      0.813
Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_2_post_val42
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8133375576554807
After post fine-tuning validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
1.7924759478681729
After Pruning
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 42094706 parameters, 74176 gradients, 158.8 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.864      0.936      0.804
Speed: 0.1ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_3_pre_val36
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_3_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_3_finetune36
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 4: MACs=79.5541908 G, #Params=42.117503 M, mAP=0.8043294271876973, speed up=1.0398749024796818</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_3_finetune36/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_3_finetune36
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.9G     0.5395     0.3534      0.897        122        640: 100%|██████████| 8/8 [00:42
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.875      0.937      0.808

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      13.2G     0.4523     0.2943     0.8601        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.875      0.942      0.816

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      13.2G     0.5011     0.3242      0.885        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.879       0.94      0.816

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      13.2G     0.4896     0.3239      0.881         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.873      0.941      0.818

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      13.2G     0.4877     0.3266     0.8653         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.924      0.883      0.939      0.819

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      13.2G     0.5175      0.341     0.8913        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.879      0.943      0.824

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      13.2G     0.5484     0.3518     0.8896         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.882      0.944      0.825

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      13.2G     0.5657     0.3636      0.901        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.947      0.877      0.944      0.827

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      13.2G     0.5557     0.3553      0.908        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.888      0.945      0.827

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      13.2G     0.6072      0.381     0.9066        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.938      0.885      0.945      0.827

10 epochs completed in 0.037 hours.
Optimizer stripped from runs/detect/step_3_finetune36/weights/last.pt, 169.0MB
Optimizer stripped from runs/detect/step_3_finetune36/weights/best.pt, 169.0MB

Validating runs/detect/step_3_finetune36/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.947      0.877      0.944      0.827
Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_3_finetune36
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.875      0.943      0.824
Speed: 0.2ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_3_post_val34
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8242263974664863
After post fine-tuning validation
Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
3.1368842425083825
After Pruning
Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 40919781 parameters, 74176 gradients, 154.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.913      0.876      0.935      0.792
Speed: 0.2ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_4_pre_val32
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_4_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_4_finetune32
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 5: MACs=77.3600192 G, #Params=40.942254 M, mAP=0.7920074671210469, speed up=1.0693690003634333</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_4_finetune32/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_4_finetune32
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.8G      0.573     0.3665     0.9011        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.926      0.881       0.94      0.803

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      13.4G     0.4596     0.2974     0.8554        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.879      0.943      0.815

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      13.2G     0.5037     0.3324     0.8775        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.875       0.94      0.815

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      13.3G     0.4863      0.313     0.8774         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.954      0.872      0.943      0.813

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      13.2G     0.4905     0.3254     0.8586         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942       0.87      0.939      0.812

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      13.2G     0.5016     0.3312     0.8863        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.914      0.888      0.939      0.812

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      13.2G     0.5446     0.3534     0.8808         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.937      0.874      0.942      0.818

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      13.2G      0.554     0.3697     0.8957        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.927      0.885      0.942      0.821

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      13.2G     0.5756      0.359     0.9062        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.889      0.945      0.825

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      13.2G     0.6089     0.3807     0.9001        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.884      0.945      0.826

10 epochs completed in 0.035 hours.
Optimizer stripped from runs/detect/step_4_finetune32/weights/last.pt, 164.3MB
Optimizer stripped from runs/detect/step_4_finetune32/weights/best.pt, 164.3MB

Validating runs/detect/step_4_finetune32/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.884      0.945      0.827
Speed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.3ms postprocess per image
Results saved to runs/detect/step_4_finetune32
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.929      0.891      0.944       0.82
Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_4_post_val31
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.820206153122929
After post fine-tuning validation
Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
5.101267981852869
After Pruning
Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 39455305 parameters, 74176 gradients, 149.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.916      0.864      0.929      0.789
Speed: 0.2ms preprocess, 13.0ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_5_pre_val31
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_5_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_5_finetune31
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 6: MACs=74.8418608 G, #Params=39.477376 M, mAP=0.7891253582912163, speed up=1.1053494062777232</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_5_finetune31/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_5_finetune31
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.5G     0.5773     0.3687     0.8973        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.904      0.881      0.935      0.801

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10        13G     0.4697     0.3058     0.8551        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.906      0.893      0.941      0.807

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      12.9G     0.5138     0.3263     0.8829        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.92      0.889      0.942      0.811

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10        13G     0.4938     0.3293     0.8823         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.928      0.883      0.944      0.813

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      12.9G     0.5047     0.3398     0.8637         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934       0.88      0.943      0.819

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10        13G     0.5144     0.3483      0.886        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.869       0.94      0.815

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10        13G      0.539     0.3521      0.879         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.943      0.865      0.941      0.814

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10        13G     0.5493     0.3683      0.895        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934      0.875      0.939      0.817

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10        13G      0.581     0.3607     0.9096        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.876       0.94       0.82

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      12.9G     0.6293     0.3874     0.9156        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.938      0.886      0.946      0.822

10 epochs completed in 0.035 hours.
Optimizer stripped from runs/detect/step_5_finetune31/weights/last.pt, 158.5MB
Optimizer stripped from runs/detect/step_5_finetune31/weights/best.pt, 158.5MB

Validating runs/detect/step_5_finetune31/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.938      0.886      0.946      0.822
Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_5_finetune31
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.928      0.891      0.943       0.82
Speed: 0.2ms preprocess, 12.9ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_5_post_val31
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8197008467567249
After post fine-tuning validation
Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
7.518590641324997
After Pruning
Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 37708749 parameters, 74176 gradients, 143.2 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.904      0.848      0.923       0.76
Speed: 0.2ms preprocess, 10.9ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_6_pre_val28
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_6_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_6_finetune28
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 7: MACs=71.732976 G, #Params=37.730325 M, mAP=0.7604747253578685, speed up=1.1532549046898597</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_6_finetune28/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_6_finetune28
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      13.3G     0.6267     0.3973     0.9214        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.909      0.859      0.932      0.782

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      12.8G     0.5114     0.3263     0.8662        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.874      0.939      0.803

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      12.7G     0.5466     0.3434     0.8876        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.931      0.875      0.939      0.808

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      12.7G     0.5238     0.3378     0.8878         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.874      0.939       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      12.7G     0.5198     0.3522     0.8708         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.947       0.87       0.94      0.808

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      12.8G     0.5276      0.352     0.8876        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.933      0.879      0.939       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      12.8G     0.5516     0.3594     0.8792         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.931      0.882      0.943       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      12.8G     0.5795     0.3736     0.9109        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.933      0.893      0.949      0.817

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      12.8G      0.597     0.3801     0.9136        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932       0.89      0.948      0.815

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      12.7G     0.6406     0.3889     0.9059        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.928      0.899      0.948      0.816

10 epochs completed in 0.034 hours.
Optimizer stripped from runs/detect/step_6_finetune28/weights/last.pt, 151.5MB
Optimizer stripped from runs/detect/step_6_finetune28/weights/best.pt, 151.5MB

Validating runs/detect/step_6_finetune28/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.933      0.889      0.948      0.817
Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_6_finetune28
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.937      0.878      0.946      0.808
Speed: 0.1ms preprocess, 10.8ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_6_post_val27</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8082043641470185
After post fine-tuning validation
Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
9.935913300797125
After Pruning
Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 35995675 parameters, 74176 gradients, 136.7 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.838      0.847      0.905      0.744
Speed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_7_pre_val25
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_7_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_7_finetune25
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 8: MACs=68.4860368 G, #Params=36.016747 M, mAP=0.7439133908787243, speed up=1.207930992438447</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_7_finetune25/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_7_finetune25
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10        13G     0.6576     0.4219     0.9433        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.884      0.852      0.921      0.764

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      13.2G     0.5285     0.3538     0.8714        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.921      0.864      0.937      0.782

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      12.4G     0.5672     0.3781     0.8972        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.921       0.87       0.94      0.791

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      12.4G     0.5324     0.3593     0.8898         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.921      0.869      0.937      0.796

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      12.1G     0.5564      0.395     0.8841         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.926      0.888      0.941      0.799

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      12.3G     0.5555     0.3674     0.9059        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.92      0.891      0.942      0.797

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      12.3G     0.5972     0.3946     0.9014         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.909      0.897      0.942      0.797

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      12.3G     0.6033     0.4048     0.9106        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.919      0.892      0.943      0.805

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      12.3G     0.6098     0.3878     0.9253        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.933      0.884      0.944      0.808

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      12.3G     0.6518     0.4124     0.9181        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934      0.887      0.945      0.811

10 epochs completed in 0.036 hours.
Optimizer stripped from runs/detect/step_7_finetune25/weights/last.pt, 144.6MB
Optimizer stripped from runs/detect/step_7_finetune25/weights/best.pt, 144.6MB

Validating runs/detect/step_7_finetune25/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934      0.887      0.945       0.81
Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_7_finetune25
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934      0.877      0.942      0.805
Speed: 0.1ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_7_post_val25</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8047311680941978
After post fine-tuning validation
Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
11.900297040141613
After Pruning
Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 34583399 parameters, 74176 gradients, 131.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.861      0.846      0.915      0.747
Speed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_8_pre_val24
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_8_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_8_finetune24
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 9: MACs=65.8289424 G, #Params=34.604045 M, mAP=0.746892685800743, speed up=1.2566874597092115</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_8_finetune24/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_8_finetune24
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      12.7G     0.6527     0.4186     0.9399        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.878       0.86      0.925      0.769

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      12.9G     0.5123     0.3376     0.8642        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.902      0.884      0.932       0.78

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      12.2G     0.5575     0.3672     0.8903        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.918      0.887      0.935      0.784

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      12.2G     0.5313     0.3422     0.8975         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.913      0.897      0.936      0.795

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      12.2G      0.543     0.3699      0.874         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.922      0.891      0.939      0.795

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      12.2G     0.5544     0.3693        0.9        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.936      0.885      0.938      0.798

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      12.2G     0.5915     0.3854     0.8924         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.883      0.939      0.801

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10        12G     0.6192     0.4081     0.9123        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.882       0.94      0.803

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      12.2G     0.6259     0.4123     0.9284        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.947       0.88      0.941      0.806

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      12.1G     0.6654     0.4213     0.9262        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.886       0.94      0.808

10 epochs completed in 0.037 hours.
Optimizer stripped from runs/detect/step_8_finetune24/weights/last.pt, 139.0MB
Optimizer stripped from runs/detect/step_8_finetune24/weights/best.pt, 139.0MB

Validating runs/detect/step_8_finetune24/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.948      0.882       0.94      0.808
Speed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_8_finetune24
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.885       0.94      0.804
Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_8_post_val24
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8042272329558376
After post fine-tuning validation
Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
13.24470533478182
After Pruning
Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 33747610 parameters, 74176 gradients, 128.5 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.919      0.872      0.923      0.774
Speed: 0.2ms preprocess, 13.1ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_9_pre_val23
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_9_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_9_finetune23
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 10: MACs=64.3900056 G, #Params=33.768007 M, mAP=0.77353892505729, speed up=1.2847709148203583</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_9_finetune23/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_9_finetune23
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      12.6G     0.6022     0.3899     0.9207        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.921      0.881       0.93      0.784

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      12.1G     0.4755     0.3118      0.851        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.941      0.883      0.933      0.794

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10        12G     0.5226     0.3441     0.8847        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.936      0.884      0.936      0.795

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      11.8G     0.5197     0.3324     0.8815         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.935      0.883      0.933      0.792

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10        12G     0.5239      0.353     0.8671         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.936      0.882      0.934      0.792

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10        12G     0.5413     0.3589     0.8919        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.93      0.877      0.934      0.802

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      12.1G     0.5753     0.3723     0.8863         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.873      0.933      0.802

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      12.1G     0.6104     0.3991     0.9113        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.95      0.868      0.931        0.8

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      12.1G     0.6059      0.395     0.9182        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.943      0.873      0.932      0.805

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10        12G     0.6558     0.4098     0.9218        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.871      0.933      0.805

10 epochs completed in 0.033 hours.
Optimizer stripped from runs/detect/step_9_finetune23/weights/last.pt, 135.6MB
Optimizer stripped from runs/detect/step_9_finetune23/weights/best.pt, 135.6MB

Validating runs/detect/step_9_finetune23/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.871      0.933      0.806
Speed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_9_finetune23</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.943      0.875      0.932      0.804
Speed: 0.2ms preprocess, 14.0ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_9_post_val23
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8042200149576527
After post fine-tuning validation
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
14.060228108679125
After Pruning
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 33209910 parameters, 74176 gradients, 126.7 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.933      0.855      0.928      0.782
Speed: 0.2ms preprocess, 13.6ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_10_pre_val17
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_10_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_10_finetune17
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 11: MACs=63.4942128 G, #Params=33.230145 M, mAP=0.7824563352367453, speed up=1.302896795658832</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_10_finetune17/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_10_finetune17
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      12.3G     0.5909     0.3739      0.911        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.938      0.863      0.931      0.795

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      11.9G     0.4459     0.2951     0.8389        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942       0.87      0.932      0.802

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      11.8G     0.5232     0.3395     0.8787        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.946      0.875      0.935      0.802

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      11.8G     0.4976     0.3318      0.877         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.878      0.934      0.795

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      11.8G     0.5079     0.3425     0.8612         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.946      0.875      0.934      0.798

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      11.8G     0.5287     0.3422     0.8902        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.951      0.875      0.937      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      11.9G     0.5654     0.3632     0.8837         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.951      0.879      0.938      0.803

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      11.9G     0.5918     0.3874     0.9027        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.948      0.878      0.937      0.802

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      11.9G     0.6008     0.3761      0.914        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.953      0.876      0.939      0.807

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      11.8G     0.6525     0.4039     0.9107        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.951      0.876      0.939      0.806

10 epochs completed in 0.032 hours.
Optimizer stripped from runs/detect/step_10_finetune17/weights/last.pt, 133.4MB
Optimizer stripped from runs/detect/step_10_finetune17/weights/best.pt, 133.4MB

Validating runs/detect/step_10_finetune17/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.952      0.876      0.937      0.807
Speed: 0.1ms preprocess, 4.3ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_10_finetune17
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.949      0.873      0.938      0.803
Speed: 0.2ms preprocess, 14.4ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_10_post_val17
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8030008974391184
After post fine-tuning validation
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
14.519222631100824
After Pruning
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32703049 parameters, 74176 gradients, 124.6 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.926      0.871      0.929      0.785
Speed: 0.1ms preprocess, 15.1ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_11_pre_val15
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_11_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_11_finetune15
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 12: MACs=62.4345712 G, #Params=32.723122 M, mAP=0.7849986248769537, speed up=1.3250096030130178</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_11_finetune15/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_11_finetune15
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      12.4G      0.592     0.3808     0.9108        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.912      0.894      0.937      0.794

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      12.5G     0.4274     0.2822     0.8394        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.936       0.88       0.94      0.808

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      11.6G     0.4962     0.3279     0.8649        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934      0.885      0.942      0.807

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      11.6G     0.4768     0.3227     0.8737         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.946      0.877      0.938      0.807

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      11.5G     0.4901     0.3294     0.8562         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.884      0.939      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      11.5G     0.5087     0.3373     0.8861        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.883      0.939       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      11.5G     0.5481     0.3556     0.8798         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.919       0.89      0.936      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      11.6G     0.5694     0.3718     0.8979        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.937      0.888      0.938      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      11.6G     0.5756     0.3754     0.9038        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.886      0.939      0.806

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      11.5G     0.6536     0.3981     0.9224        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.95      0.883      0.939      0.809

10 epochs completed in 0.030 hours.
Optimizer stripped from runs/detect/step_11_finetune15/weights/last.pt, 131.4MB
Optimizer stripped from runs/detect/step_11_finetune15/weights/best.pt, 131.4MB

Validating runs/detect/step_11_finetune15/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.883      0.939      0.809
Speed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_11_finetune15
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.885      0.938      0.803
Speed: 0.2ms preprocess, 15.9ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_11_post_val14
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8028105881367777
After post fine-tuning validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
14.766719382862217
After Pruning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32669140 parameters, 74176 gradients, 124.6 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.883      0.942      0.806
Speed: 0.1ms preprocess, 15.9ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_12_pre_val14
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_12_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_12_finetune13
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 13: MACs=62.4070664 G, #Params=32.689204 M, mAP=0.8058915915724488, speed up=1.325593577332454</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_12_finetune13/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_12_finetune13
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      12.7G      0.499     0.3319     0.8801        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.933      0.887      0.937      0.809

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      11.6G     0.3689     0.2572     0.8209        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.932      0.886      0.938      0.812

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      11.6G     0.4539      0.302     0.8599        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.949      0.876      0.938       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      11.6G     0.4334     0.2969     0.8595         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.94      0.893      0.941      0.808

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      11.6G     0.4529     0.3123     0.8466         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.894      0.941      0.807

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      11.6G     0.4631     0.3128     0.8697        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.95      0.887      0.943      0.809

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      11.6G     0.5213     0.3408     0.8692         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.941      0.891      0.939      0.811

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      11.7G      0.539     0.3604     0.8853        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.892       0.94       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      11.7G     0.5515      0.358     0.8976        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.929      0.897       0.94      0.813

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      11.6G     0.6402     0.3891     0.9106        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.941      0.897      0.943      0.816

10 epochs completed in 0.029 hours.
Optimizer stripped from runs/detect/step_12_finetune13/weights/last.pt, 131.3MB
Optimizer stripped from runs/detect/step_12_finetune13/weights/best.pt, 131.3MB

Validating runs/detect/step_12_finetune13/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.942      0.897      0.944      0.816
Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_12_finetune13</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.937      0.892      0.941      0.811
Speed: 0.2ms preprocess, 16.7ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_12_post_val13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8111457220640336
After post fine-tuning validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
14.89709551315643
After Pruning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.939      0.886       0.94      0.805
Speed: 0.2ms preprocess, 16.6ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_13_pre_val13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_13_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_13_finetune13
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...
AMP: checks passed ✅</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 14: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8050285863373501, speed up=1.3375568226839933</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_13_finetune13/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_13_finetune13
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      12.1G     0.5096      0.332     0.8815        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.935      0.891      0.943      0.812

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      11.6G     0.3826     0.2558     0.8262        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.956       0.88      0.949      0.813

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      11.4G     0.4512     0.3126     0.8555        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.948      0.883      0.945      0.814

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      11.5G     0.4423      0.299     0.8562         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.95      0.882      0.944      0.811

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      11.5G     0.4557     0.3122     0.8492         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.935      0.896      0.945      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      11.5G     0.4734     0.3233     0.8728        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.93      0.892      0.941      0.805

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      11.5G     0.5367      0.352     0.8796         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.934      0.886      0.942      0.804

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      11.5G     0.5403     0.3508     0.8848        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.941      0.886      0.943       0.81

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      11.5G     0.5416     0.3534      0.889        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.893      0.947      0.814

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      11.5G     0.6465     0.4027     0.9187        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.945      0.891      0.947      0.818

10 epochs completed in 0.030 hours.
Optimizer stripped from runs/detect/step_13_finetune13/weights/last.pt, 130.3MB
Optimizer stripped from runs/detect/step_13_finetune13/weights/best.pt, 130.3MB

Validating runs/detect/step_13_finetune13/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.891      0.947      0.819
Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_13_finetune13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.951      0.887      0.946      0.815
Speed: 0.2ms preprocess, 17.3ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_13_post_val13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8146198835662797
After post fine-tuning validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
14.96493134246744
After Pruning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.951      0.886      0.945      0.815
Speed: 0.2ms preprocess, 17.4ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_14_pre_val13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
yolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_14_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_14_finetune13
AMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After post-pruning Validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
After pruning iter 15: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8153698637584581, speed up=1.3375568226839933</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>AMP: checks passed ✅
train: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
Plotting labels to runs/detect/step_14_finetune13/labels.jpg... 
optimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to runs/detect/step_14_finetune13
Starting training for 10 epochs...
Closing dataloader mosaic

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       1/10      11.4G     0.4922     0.3236     0.8733        122        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.955      0.882      0.946      0.822

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       2/10      11.4G     0.3523     0.2478     0.8197        112        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.953       0.89      0.943      0.817

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       3/10      11.4G     0.4283     0.2858     0.8487        116        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929       0.95       0.89      0.941       0.82

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       4/10      11.4G      0.408     0.2829     0.8446         68        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.953      0.886      0.945      0.819

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       5/10      11.4G     0.4299     0.2959     0.8395         96        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.943      0.902      0.944      0.823

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       6/10      11.4G     0.4327     0.3007     0.8581        120        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.946      0.889      0.943       0.82

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       7/10      11.4G     0.4762     0.3178     0.8584         69        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.944      0.892      0.945      0.814

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       8/10      11.4G     0.5052     0.3337     0.8724        141        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.946      0.893      0.944      0.819

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
       9/10      11.4G     0.5167     0.3323     0.8776        104        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.943      0.898      0.944      0.819

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
      10/10      11.4G     0.5994     0.3758     0.8965        170        640: 100%|██████████| 8/8 [00:02
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.952      0.893      0.946      0.822

10 epochs completed in 0.017 hours.
Optimizer stripped from runs/detect/step_14_finetune13/weights/last.pt, 130.3MB
Optimizer stripped from runs/detect/step_14_finetune13/weights/best.pt, 130.3MB

Validating runs/detect/step_14_finetune13/weights/best.pt...
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)
YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.943      0.902      0.944      0.823
Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image
Results saved to runs/detect/step_14_finetune13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine-tuning
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|
                   all        128        929      0.947      0.899      0.943      0.819
Speed: 0.2ms preprocess, 17.2ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/step_14_post_val13
Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CPU</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>After fine tuning mAP=0.8192818206570706
After post fine-tuning validation
Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs

PyTorch: starting from runs/detect/step_14_finetune13/weights/best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (124.2 MB)

ONNX: starting export with onnx 1.16.0 opset 17...
ONNX: export success ✅ 2.6s, saved as runs/detect/step_14_finetune13/weights/best.onnx (123.9 MB)

Export complete (3.5s)
Results saved to /home/HubensN/fasterai/nbs/runs/detect/step_14_finetune13/weights
Predict:         yolo predict task=detect model=runs/detect/step_14_finetune13/weights/best.onnx imgsz=640 
Validate:        yolo val task=detect model=runs/detect/step_14_finetune13/weights/best.onnx imgsz=640 data=/home/HubensN/miniconda3/envs/fasterai/lib/python3.9/site-packages/ultralytics/datasets/coco128.yaml 
Visualize:       https://netron.app</code></pre>
</div>
</div>
</section>
<section id="post-training-checks" class="level2">
<h2 class="anchored" data-anchor-id="post-training-checks">Post-Training Checks</h2>
<div id="17c4390e-038c-42a5-a80a-872e35bf1b9b" class="cell">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'/home/HubensN/fasterai/nbs/runs/detect/step_14_finetune4/weights/best.pt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5d3527bf-bdca-40f0-93d3-42af28b01b6e" class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>base_macs, base_nparams <span class="op">=</span> tp.utils.count_ops_and_params(model.model, example_inputs)<span class="op">;</span> base_macs, base_nparams</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(57692198400.0, 30077028)</code></pre>
</div>
</div>
<div id="39190b40-3d3b-4844-9da7-c41e9b9aa6c9" class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> model.val(</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>                data<span class="op">=</span><span class="st">'coco128.yaml'</span>,</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>                batch<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>                imgsz<span class="op">=</span><span class="dv">640</span>,</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>                verbose<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>            )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)
YOLOv8l summary (fused): 285 layers, 30057792 parameters, 0 gradients, 115.1 GFLOPs
val: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/tra
                 Class     Images  Instances      Box(P          R  
                   all        128        929      0.917      0.907      0.945      0.809
Speed: 0.2ms preprocess, 24.4ms inference, 0.0ms loss, 0.4ms postprocess per image
Results saved to runs/detect/val35</code></pre>
</div>
</div>
<div id="feb54734-cf16-4051-ae58-1e0b03435c3d" class="cell">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>ultralytics.yolo.utils.metrics.DetMetrics object with attributes:

ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])
box: ultralytics.yolo.utils.metrics.Metric object
confusion_matrix: &lt;ultralytics.yolo.utils.metrics.ConfusionMatrix object&gt;
fitness: 0.8221835652536718
keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']
maps: array([    0.75668,     0.47387,      0.3594,     0.91994,     0.94661,     0.90211,     0.94289,     0.68531,     0.68927,     0.34436,     0.80851,      0.8955,     0.80851,     0.86091,     0.84563,     0.96863,     0.89911,      0.9501,     0.80851,     0.80851,     0.88959,       0.995,       0.995,     0.93382,
            0.8273,     0.84511,      0.6686,     0.77723,     0.80558,      0.8234,      0.8955,     0.68357,     0.40784,     0.61583,     0.67229,     0.39977,     0.87814,     0.80851,     0.60498,     0.56705,       0.726,     0.76821,     0.76074,     0.56956,     0.72918,     0.79545,       0.995,     0.80851,
             0.995,      0.8713,     0.73808,      0.7727,       0.995,     0.94354,     0.96781,      0.9387,     0.81318,     0.93981,     0.90203,       0.995,     0.80108,     0.90138,       0.995,      0.9641,     0.55702,     0.69798,     0.80851,     0.74254,     0.90157,     0.90353,     0.80851,     0.80032,
           0.92735,     0.62032,     0.86949,     0.92389,       0.995,      0.9143,     0.80851,     0.94696])
names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}
plot: True
results_dict: {'metrics/precision(B)': 0.9173606393289034, 'metrics/recall(B)': 0.9071337022261394, 'metrics/mAP50(B)': 0.9452653330065343, 'metrics/mAP50-95(B)': 0.8085078132811314, 'fitness': 0.8221835652536718}
save_dir: Path('runs/detect/val35')
speed: {'preprocess': 0.17499923706054688, 'inference': 24.442605674266815, 'loss': 0.0046156346797943115, 'postprocess': 0.376259908080101}</code></pre>
</div>
</div>
<div id="ef36d64b-13ff-4480-ae1b-7d302a7857e8" class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>model.export(<span class="bu">format</span> <span class="op">=</span> <span class="st">'onnx'</span>, half <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CPU
WARNING ⚠️ half=True only compatible with GPU export, i.e. use device=0
YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs

PyTorch: starting from yolov8l.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (83.7 MB)

ONNX: starting export with onnx 1.16.0 opset 17...
ONNX: export success ✅ 2.8s, saved as yolov8l.onnx (166.8 MB)

Export complete (4.0s)
Results saved to /home/HubensN/fasterai/nbs
Predict:         yolo predict task=detect model=yolov8l.onnx imgsz=640 
Validate:        yolo val task=detect model=yolov8l.onnx imgsz=640 data=coco.yaml 
Visualize:       https://netron.app</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>'yolov8l.onnx'</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/nathanhubens\.github\.io\/fasterai\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© By Nathan Hubens</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/nathanhubens/fasterai/tree/master/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>