[
  {
    "objectID": "tutorial.lottery_ticket.html",
    "href": "tutorial.lottery_ticket.html",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B &gt; a_A\\), in a training time \\(t_B &lt; t_A\\).\nLet’s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLet’s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.595053\n0.543628\n0.717185\n00:07\n\n\n1\n0.546125\n0.531834\n0.728687\n00:07\n\n\n2\n0.521566\n0.521680\n0.746279\n00:07\n\n\n3\n0.478598\n0.478660\n0.764547\n00:07\n\n\n4\n0.421515\n0.719835\n0.633288\n00:07\n\n\n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found “lottery ticket” after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.587271\n0.612850\n0.668471\n00:07\n\n\n1\n0.547513\n0.573875\n0.701624\n00:07\n\n\n2\n0.501447\n0.514319\n0.738160\n00:07\n\n\n3\n0.467517\n0.518835\n0.740189\n00:07\n\n\n4\n0.431308\n0.621117\n0.654263\n00:07\n\n\n5\n0.561822\n0.546851\n0.728011\n00:07\n\n\n6\n0.514754\n0.570669\n0.765223\n00:07\n\n\n7\n0.469772\n0.576346\n0.675913\n00:07\n\n\n8\n0.427512\n0.454539\n0.799053\n00:07\n\n\n9\n0.385058\n0.537472\n0.728011\n00:07\n\n\n10\n0.482251\n0.442526\n0.778078\n00:07\n\n\n11\n0.430115\n0.510421\n0.777402\n00:07\n\n\n12\n0.383115\n0.424600\n0.811908\n00:07\n\n\n13\n0.341061\n0.663934\n0.681326\n00:07\n\n\n14\n0.322339\n0.383250\n0.830853\n00:07\n\n\n15\n0.402276\n0.441383\n0.799053\n00:07\n\n\n16\n0.348916\n0.384334\n0.814614\n00:07\n\n\n17\n0.311075\n0.409829\n0.817321\n00:07\n\n\n18\n0.280943\n0.405584\n0.824763\n00:07\n\n\n19\n0.244700\n0.385061\n0.827470\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B &gt; a_A\\) in the same training time.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorial.lottery_ticket.html#the-lottery-ticket-hypothesis",
    "href": "tutorial.lottery_ticket.html#the-lottery-ticket-hypothesis",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B &gt; a_A\\), in a training time \\(t_B &lt; t_A\\).\nLet’s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLet’s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.595053\n0.543628\n0.717185\n00:07\n\n\n1\n0.546125\n0.531834\n0.728687\n00:07\n\n\n2\n0.521566\n0.521680\n0.746279\n00:07\n\n\n3\n0.478598\n0.478660\n0.764547\n00:07\n\n\n4\n0.421515\n0.719835\n0.633288\n00:07\n\n\n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found “lottery ticket” after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.587271\n0.612850\n0.668471\n00:07\n\n\n1\n0.547513\n0.573875\n0.701624\n00:07\n\n\n2\n0.501447\n0.514319\n0.738160\n00:07\n\n\n3\n0.467517\n0.518835\n0.740189\n00:07\n\n\n4\n0.431308\n0.621117\n0.654263\n00:07\n\n\n5\n0.561822\n0.546851\n0.728011\n00:07\n\n\n6\n0.514754\n0.570669\n0.765223\n00:07\n\n\n7\n0.469772\n0.576346\n0.675913\n00:07\n\n\n8\n0.427512\n0.454539\n0.799053\n00:07\n\n\n9\n0.385058\n0.537472\n0.728011\n00:07\n\n\n10\n0.482251\n0.442526\n0.778078\n00:07\n\n\n11\n0.430115\n0.510421\n0.777402\n00:07\n\n\n12\n0.383115\n0.424600\n0.811908\n00:07\n\n\n13\n0.341061\n0.663934\n0.681326\n00:07\n\n\n14\n0.322339\n0.383250\n0.830853\n00:07\n\n\n15\n0.402276\n0.441383\n0.799053\n00:07\n\n\n16\n0.348916\n0.384334\n0.814614\n00:07\n\n\n17\n0.311075\n0.409829\n0.817321\n00:07\n\n\n18\n0.280943\n0.405584\n0.824763\n00:07\n\n\n19\n0.244700\n0.385061\n0.827470\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B &gt; a_A\\) in the same training time.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorial.lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "href": "tutorial.lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "title": "Lottery Ticket Hypothesis",
    "section": "Lottery Ticket Hypothesis with Rewinding",
    "text": "Lottery Ticket Hypothesis with Rewinding\nIn some case, LTH fails for deeper networks, author then propose a solution, which is to rewind the weights to a more advanced iteration instead of the initialization value.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nThis can be done in fasterai by passing the rewind_epoch parameter, that will save the weights at that epoch, then resetting the weights accordingly.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, rewind_epoch=1)\n\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.594580\n0.625727\n0.685386\n00:07\n\n\n1\n0.543424\n0.667005\n0.675237\n00:07\n\n\n2\n0.509345\n0.483691\n0.748309\n00:07\n\n\n3\n0.470524\n0.478201\n0.763870\n00:07\n\n\n4\n0.451323\n0.550612\n0.725981\n00:07\n\n\n5\n0.518282\n0.483968\n0.762517\n00:07\n\n\n6\n0.478035\n0.729290\n0.709743\n00:07\n\n\n7\n0.425959\n0.479093\n0.782815\n00:07\n\n\n8\n0.393114\n0.407376\n0.815968\n00:07\n\n\n9\n0.350740\n0.755570\n0.654939\n00:07\n\n\n10\n0.442296\n0.497535\n0.759134\n00:07\n\n\n11\n0.390260\n0.617039\n0.677267\n00:07\n\n\n12\n0.346842\n0.365099\n0.836942\n00:07\n\n\n13\n0.311359\n0.431787\n0.811908\n00:07\n\n\n14\n0.276918\n0.389922\n0.835589\n00:07\n\n\n15\n0.342124\n0.495095\n0.748985\n00:07\n\n\n16\n0.308753\n0.553223\n0.734777\n00:07\n\n\n17\n0.278255\n0.491012\n0.803112\n00:07\n\n\n18\n0.254106\n0.564554\n0.782815\n00:07\n\n\n19\n0.224321\n0.341632\n0.855210\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSaving Weights at epoch 1\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorial.lottery_ticket.html#super-masks",
    "href": "tutorial.lottery_ticket.html#super-masks",
    "title": "Lottery Ticket Hypothesis",
    "section": "Super-Masks",
    "text": "Super-Masks\nResearchers from Uber AI investigated the LTH and found the existence of what they call “Super-Masks”, i.e. masks that, applied on a untrained neural network, allows to reach better-than-random results.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nTo find supermasks, authors perform the LTH method then apply the mask on the original, untrained network. In fasterai, you can pass the parameter reset_end=True, which will reset the weights to their original value at the end of the training, but keeping the pruned weights (i.e. the mask) unchanged.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, reset_end=True)\n\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.611311\n0.796523\n0.674560\n00:07\n\n\n1\n0.560894\n0.608468\n0.685386\n00:07\n\n\n2\n0.529301\n0.537641\n0.736130\n00:08\n\n\n3\n0.476619\n0.542131\n0.740866\n00:07\n\n\n4\n0.441745\n0.456982\n0.776725\n00:07\n\n\n5\n0.538183\n0.558080\n0.737483\n00:07\n\n\n6\n0.497710\n0.502065\n0.749662\n00:07\n\n\n7\n0.457478\n0.542794\n0.731394\n00:07\n\n\n8\n0.425738\n0.548435\n0.725981\n00:07\n\n\n9\n0.393300\n0.562673\n0.680650\n00:07\n\n\n10\n0.475655\n0.520430\n0.756428\n00:08\n\n\n11\n0.433569\n0.582280\n0.740866\n00:08\n\n\n12\n0.408267\n0.755556\n0.547361\n00:08\n\n\n13\n0.364271\n0.421615\n0.809202\n00:07\n\n\n14\n0.324698\n0.368889\n0.832882\n00:07\n\n\n15\n0.394710\n0.604650\n0.738836\n00:07\n\n\n16\n0.350399\n0.412777\n0.814614\n00:07\n\n\n17\n0.318693\n0.443487\n0.803789\n00:07\n\n\n18\n0.280467\n0.395299\n0.826793\n00:07\n\n\n19\n0.251164\n0.380674\n0.831529\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\n\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [0.8659006357192993,0.665764570236206]",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorial.prune_callback.html",
    "href": "tutorial.prune_callback.html",
    "title": "Prune Callback",
    "section": "",
    "text": "Let’s try our PruneCallback on the Pets dataset\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nWe’ll train a vanilla ResNet18 for 5 epochs to have an idea of the expected performance\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.653510\n0.805452\n0.831529\n00:08\n\n\n1\n0.373264\n0.246071\n0.901894\n00:03\n\n\n2\n0.226383\n0.212931\n0.912043\n00:03\n\n\n3\n0.118254\n0.186566\n0.920162\n00:03\n\n\n4\n0.067994\n0.185255\n0.924899\n00:03\n\n\n\n\n\n\nbase_macs, base_params = tp.utils.count_ops_and_params(learn.model, torch.randn(1,3,224,224).to(default_device()))\n\nLet’s now try adding to remove some filters in our model\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nWe’ll set the sparsity to 50 (i.e. remove 50% of filters), the context to global (i.e. we remove filters from anywhere in the network), the criteria to large_final (i.e. keep the highest value filters and the schedule to one_cycle (i.e. follow the One-Cycle schedule to remove filters along training).\n\npr_cb = PruneCallback(sparsity=45, context='global', criteria=large_final, schedule=one_cycle, layer_type=[nn.Conv2d])\nlearn.fit_one_cycle(10, cbs=pr_cb)\n\nPruning until a sparsity of [45]%\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.923606\n0.602696\n0.727334\n00:04\n\n\n1\n0.524770\n0.355539\n0.857239\n00:05\n\n\n2\n0.352097\n0.269183\n0.890392\n00:05\n\n\n3\n0.270549\n0.325706\n0.890392\n00:05\n\n\n4\n0.205549\n0.192651\n0.920162\n00:06\n\n\n5\n0.155798\n0.221400\n0.908660\n00:05\n\n\n6\n0.137832\n0.197844\n0.907984\n00:05\n\n\n7\n0.109144\n0.196927\n0.924222\n00:05\n\n\n8\n0.085867\n0.183181\n0.933694\n00:05\n\n\n9\n0.084885\n0.186639\n0.927605\n00:05\n\n\n\n\n\nSparsity at the end of epoch 0: [0.45]%\nSparsity at the end of epoch 1: [1.76]%\nSparsity at the end of epoch 2: [6.39]%\nSparsity at the end of epoch 3: [18.07]%\nSparsity at the end of epoch 4: [32.91]%\nSparsity at the end of epoch 5: [41.27]%\nSparsity at the end of epoch 6: [44.03]%\nSparsity at the end of epoch 7: [44.77]%\nSparsity at the end of epoch 8: [44.95]%\nSparsity at the end of epoch 9: [45.0]%\nFinal Sparsity: [45.0]%\n\n\n\npruned_macs, pruned_params = tp.utils.count_ops_and_params(learn.model, torch.randn(1,3,224,224).to(default_device()))\n\nWe observe that our network has lost 2.5% of accuracy. But how much parameters have we removed and how much compute does that save ?\n\nprint(f'The pruned model has {pruned_macs/base_macs:.2f} the compute of original model')\n\nThe pruned model has 0.75 the compute of original model\n\n\n\nprint(f'The pruned model has {pruned_params/base_params:.2f} the parameters of original model')\n\nThe pruned model has 0.26 the parameters of original model\n\n\nSo at the price of a slight decrease in accuracy, we now have a model that is 5x smaller and requires 1.5x fewer compute.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorial.fc_decomposer.html#get-the-data",
    "href": "tutorial.fc_decomposer.html#get-the-data",
    "title": "Fully-Connected layers decomposition",
    "section": "1. Get the data",
    "text": "1. Get the data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorial.fc_decomposer.html#train-the-model",
    "href": "tutorial.fc_decomposer.html#train-the-model",
    "title": "Fully-Connected layers decomposition",
    "section": "2. Train the model",
    "text": "2. Train the model\n\nlearn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.886644\n0.652151\n0.685386\n00:22\n\n\n1\n0.692583\n0.627857\n0.685386\n00:21\n\n\n2\n0.646516\n0.622866\n0.685386\n00:22\n\n\n\n\n\n\nDecompose !\n\n\nfc = FC_Decomposer()\nnew_model = fc.decompose(learn.model)\n\nThe fc layers have been factorized and replace by smaller ones.\n\nnew_model\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=25088, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Sequential(\n      (0): Linear(in_features=4096, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Sequential(\n      (0): Linear(in_features=4096, out_features=1, bias=False)\n      (1): Linear(in_features=1, out_features=2, bias=True)\n    )\n  )\n)\n\n\nWe can see compare the amount of parameters before/after:\n\ncount_parameters(learn.model)\n\n134277186\n\n\n\ncount_parameters(new_model)\n\n91281476\n\n\nThis represents a decrease of ~40M parameters !\nNow this is an approximation, so it isn’t really lossless and we should expect to see a performance drop, which will be bigger as we keep fewer singular values. Here we have:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.validate()\n\n\n\n\n(#2) [0.6868855357170105,0.6853856444358826]",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorial.transformers.html",
    "href": "tutorial.transformers.html",
    "title": "Prune Transformers",
    "section": "",
    "text": "Note\n\n\n\nThis example code is taken from the fastai docs\npretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)\npath = untar_data(URLs.WIKITEXT_TINY)\nLet’s create our fastai Learner.\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\nAnd let’s try to extend a given prompt with the pretrained model.\nprompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\"\npreds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\ntokenizer.decode(preds[0].cpu().numpy())\n\n'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn on its head.\\n\\nA unicorn is a magical creature with a rainbow tail and a horn'\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [3.695716619491577,40.2744255065918]\nlearn.fit_one_cycle(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nperplexity\ntime\n\n\n\n\n0\n3.124115\n2.844266\n17.188944\n07:50\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_2855382/2352043074.py in &lt;cell line: 4&gt;()\n      2 inp = tensor(prompt_ids)[None]\n      3 \n----&gt; 4 preds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n      5 \n      6 tokenizer.decode(preds[0].cpu().numpy())\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\n     25         def decorate_context(*args, **kwargs):\n     26             with self.clone():\n---&gt; 27                 return func(*args, **kwargs)\n     28         return cast(F, decorate_context)\n     29 \n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/generation_utils.py in generate(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\n   1352             )\n   1353             # 12. run beam search\n-&gt; 1354             return self.beam_search(\n   1355                 input_ids,\n   1356                 beam_scorer,\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/generation_utils.py in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\n   2203             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n   2204 \n-&gt; 2205             outputs = self(\n   2206                 **model_inputs,\n   2207                 return_dict=True,\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\n   1046         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n   1047 \n-&gt; 1048         transformer_outputs = self.transformer(\n   1049             input_ids,\n   1050             past_key_values=past_key_values,\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\n    832 \n    833         if inputs_embeds is None:\n--&gt; 834             inputs_embeds = self.wte(input_ids)\n    835         position_embeds = self.wpe(position_ids)\n    836         hidden_states = inputs_embeds + position_embeds\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/sparse.py in forward(self, input)\n    156 \n    157     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 158         return F.embedding(\n    159             input, self.weight, self.padding_idx, self.max_norm,\n    160             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2181         # remove once script supports set_grad_enabled\n   2182         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2183     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2184 \n   2185 \n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorial.transformers.html#make-it-sparse",
    "href": "tutorial.transformers.html#make-it-sparse",
    "title": "Prune Transformers",
    "section": "Make it sparse !",
    "text": "Make it sparse !\nLet’s see now if we retrain our model, this time introducing sparsity\n\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\n\nUnfortunately, the transformer model uses a custom layer: Conv1D, which is not a part of PyTorch. To overcome this problem, we have to add this layer to our Granularities class, so that it knows what to sparsify.\nHere, the Conv1D behaves like a Linear layer, i.e. the weights are defined by a matrix of dimension (nf,nx)\n\ndoc(Conv1D)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\nConv1D\nConv1D(nf, nx)1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n\nBasically works like a linear layer but the weights are transposed.\n\nArgs:\n    nf (`int`): The number of output features.\n    nx (`int`): The number of input features.\n\n\nWe can thus add the Conv1D granularity by using the add_granularity method, indicating the target module and the corresponding granularities that it can handle (the same as Linear so we can reuse it)\n\nGranularities.add_granularity(Conv1D, Granularities._granularities_Linear)\n\nLet’s now define our SparsifyCallback. Let’s say we want to make our model 30% sparse, by removing the highest-norm weight in each attention head.\n\nsp_cb = SparsifyCallback(sparsity=30, granularity='weight', context='local', criteria=large_final, schedule=one_cycle, layer_type=Conv1D)\n\nWe now only have to pass our callback to fastai\n\nlearn.fit_one_cycle(1, 1e-4, cbs=sp_cb)\n\nPruning of weight until a sparsity of [30]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nperplexity\ntime\n\n\n\n\n0\n3.151266\n2.882525\n17.859306\n09:44\n\n\n\n\n\nSparsity at the end of epoch 0: [30.0]%\nFinal Sparsity: [30.0]%\nSparsity in Conv1D 9: 30.00%\nSparsity in Conv1D 10: 30.00%\nSparsity in Conv1D 15: 30.00%\nSparsity in Conv1D 16: 30.00%\nSparsity in Conv1D 22: 30.00%\nSparsity in Conv1D 23: 30.00%\nSparsity in Conv1D 28: 30.00%\nSparsity in Conv1D 29: 30.00%\nSparsity in Conv1D 34: 30.00%\nSparsity in Conv1D 35: 30.00%\nSparsity in Conv1D 40: 30.00%\nSparsity in Conv1D 41: 30.00%\nSparsity in Conv1D 46: 30.00%\nSparsity in Conv1D 47: 30.00%\nSparsity in Conv1D 52: 30.00%\nSparsity in Conv1D 53: 30.00%\nSparsity in Conv1D 58: 30.00%\nSparsity in Conv1D 59: 30.00%\nSparsity in Conv1D 64: 30.00%\nSparsity in Conv1D 65: 30.00%\nSparsity in Conv1D 70: 30.00%\nSparsity in Conv1D 71: 30.00%\nSparsity in Conv1D 76: 30.00%\nSparsity in Conv1D 77: 30.00%\nSparsity in Conv1D 82: 30.00%\nSparsity in Conv1D 83: 30.00%\nSparsity in Conv1D 88: 30.00%\nSparsity in Conv1D 89: 30.00%\nSparsity in Conv1D 94: 30.00%\nSparsity in Conv1D 95: 30.00%\nSparsity in Conv1D 100: 30.00%\nSparsity in Conv1D 101: 30.00%\nSparsity in Conv1D 106: 30.00%\nSparsity in Conv1D 107: 30.00%\nSparsity in Conv1D 112: 30.00%\nSparsity in Conv1D 113: 30.00%\nSparsity in Conv1D 118: 30.00%\nSparsity in Conv1D 119: 30.00%\nSparsity in Conv1D 124: 30.00%\nSparsity in Conv1D 125: 30.00%\nSparsity in Conv1D 130: 30.00%\nSparsity in Conv1D 131: 30.00%\nSparsity in Conv1D 136: 30.00%\nSparsity in Conv1D 137: 30.00%\nSparsity in Conv1D 142: 30.00%\nSparsity in Conv1D 143: 30.00%\nSparsity in Conv1D 148: 30.00%\nSparsity in Conv1D 149: 30.00%\n\n\nAnd we can check the predicion to the same prompt as before\n\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn @-@ shaped head. The unicorn is a member of the &lt;unk&gt; &lt;unk&gt;'\n\n\nThat’s it ! You now have a sparse Transformer as performant as the whole model. However, this model is currently not more efficient speed and storage wise. To have such a speed-up, I suggest you to look at the granularity section.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "regularize.regularizer.html",
    "href": "regularize.regularizer.html",
    "title": "Regularize Callback",
    "section": "",
    "text": "RegularizeCallback\n\n RegularizeCallback (g, wd=0.01, layer_type=&lt;class\n                     'torch.nn.modules.conv.Conv2d'&gt;)\n\nCallback to apply grouped weight decay\nThe RegularizeCallbackcan be used to perform \\(l_1\\) regularization on any granularity available in the criteria class.",
    "crumbs": [
      "Get Started",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "sparse.sparsifier.html",
    "href": "sparse.sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network’s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\n\nsource\n\nSparsifier\n\n Sparsifier (model, granularity, context, criteria, layer_type=&lt;class\n             'torch.nn.modules.conv.Conv2d'&gt;)\n\nClass providing sparsifying capabilities\nThe Sparsifier class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n\nThe granularity, i.e. the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\nThe context, i.e. if you want to consider each layer independently (local), or compare the parameters to remove across the whole network (global).\nThe criteria, i.e. the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful.\n\nUser can pass a single layer to prune by using the Sparsifier.sparsify_layer method.\n\nsource\n\n\nSparsifier.sparsify_layer\n\n Sparsifier.sparsify_layer (m, sparsity, round_to=None)\n\nMost of the time, we may want to prune the whole model at once, using the Sparsifier.prune_model method, indicating the percentage of sparsity to you want to apply.\n\nsource\n\n\nSparsifier.sparsify_model\n\n Sparsifier.sparsify_model (sparsity, round_to=None)\n\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g. 8), this can be done by passing the round_to parameter.\n\nAlso, instead of passing a single value of sparsity, a list of sparsities can also be provided. In that case, each value in the list is the sparsity that will be applied to all layers.\nExample: I have a 4-layer network and want to remove half of the parameters from the layers 2 and 3, I can provide the list: sparsity = [0, 50, 50, 0]",
    "crumbs": [
      "Get Started",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "core.criteria.html",
    "href": "core.criteria.html",
    "title": "Criteria",
    "section": "",
    "text": "The criteria implemented come from this paper.",
    "crumbs": [
      "Get Started",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core.criteria.html#weight-based-criteria",
    "href": "core.criteria.html#weight-based-criteria",
    "title": "Criteria",
    "section": "Weight Based Criteria",
    "text": "Weight Based Criteria\n\nRandom\n\ndemo_model(random)\n\n\n\n\n\n\n\n\n\n\nLarge Final Value\n\ndemo_model(large_final)\n\n\n\n\n\n\n\n\n\n\nSquared Final Value\n\ndemo_model(squared_final)\n\n\n\n\n\n\n\n\n\n\nSmall Final Value\n\ndemo_model(small_final)\n\n\n\n\n\n\n\n\n\n\nLarge Init Value\n\ndemo_model(large_init)\n\n\n\n\n\n\n\n\n\n\nSmall Init Value\n\ndemo_model(small_init)\n\n\n\n\n\n\n\n\n\n\nLarge Init Large Final Value\n\ndemo_model(large_init_large_final, 80)\n\n\n\n\n\n\n\n\n\n\nSmall Init Small Final Value\n\ndemo_model(small_init_small_final)\n\n\n\n\n\n\n\n\n\n\nIncreasing Magnitude\n\ndemo_model(magnitude_increase, 60)\n\n\n\n\n\n\n\n\n\n\nMovement Pruning\n\ndemo_model(movement)",
    "crumbs": [
      "Get Started",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core.criteria.html#updating-versions",
    "href": "core.criteria.html#updating-versions",
    "title": "Criteria",
    "section": "Updating Versions",
    "text": "Updating Versions\nThe following criteria use an updating value of the weights, i.e. the value from the previous iteration of training, instead of the initialization value to better capture the training dynamics.\n\nUpdating Magnitude Increase\n\ndemo_model(updating_magnitude_increase)\n\n\n\n\n\n\n\n\n\n\nUpdating Movement\n\ndemo_model(updating_movement, 50)\n\n\n\n\n\n\n\n\n\n\nmov-magnitude\n\ndemo_model(movmag)\n\n\n\n\n\n\n\n\n\n\nUpdating mov-magnitude\n\ndemo_model(updating_movmag)",
    "crumbs": [
      "Get Started",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core.criteria.html#gradient-based-criteria",
    "href": "core.criteria.html#gradient-based-criteria",
    "title": "Criteria",
    "section": "Gradient Based Criteria",
    "text": "Gradient Based Criteria",
    "crumbs": [
      "Get Started",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Methods • Features • Installation • Tutorials • Citing • License\nfasterai is a library created to make neural network smaller and faster. It essentially relies on common compression techniques for networks such as pruning, knowledge distillation, Lottery Ticket Hypothesis, …\nThe core feature of fasterai is its Sparsifying capabilities, constructed around 4 main modules: granularity, context, criteria, schedule. Each of these modules is highly customizable, allowing you to change them according to your needs or even to come up with your own !",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#project-documentation",
    "href": "overview.html#project-documentation",
    "title": "Overview",
    "section": "Project Documentation",
    "text": "Project Documentation\nVisit Read The Docs Project Page or read following README to know more about using fasterai.",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#features",
    "href": "overview.html#features",
    "title": "Overview",
    "section": "Features",
    "text": "Features\n\n1. Sparsifying\n\nMake your model sparse according to a:  - Sparsity:  the percentage of weights that will be replaced by 0  - Granularity:  the granularity at which you operate the pruning (removing weights, vectors, kernels, filters)  - Context:  prune either each layer independantly (local pruning) or the whole model (global pruning)  - Criteria:  the criteria used to select the weights to remove (magnitude, movement, …)  - Schedule:  which schedule you want to use for pruning (one shot, iterative, gradual, …) \nThis can be achieved by using the SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n2. Pruning\n\nOnce your model has useless nodes due to zero-weights, they can be removed to not be a part of the network anymore.\nThis can be achieved by using the PruneCallback(sparsity, context, criteria, schedule)\n\n\n3. Regularization\n\nInstead of explicitely make your network sparse, let it train towards sparse connections by pushing the weights to be as small as possible.\nRegularization can be applied to groups of weights, following the same granularities as for sparsifying, i.e.: - Granularity:  the granularity at which you operate the regularization (weights, vectors, kernels, filters, …)\nThis can be achieved by using the RegularizationCallback(granularity)\n\n\n4. Knowledge Distillation\n\n\n\nalt text\n\n\nDistill the knowledge acquired by a big model into a smaller one, by using the KnowledgeDistillation callback.\n\n\n5. Lottery Ticket Hypothesis\n\nFind the winning ticket in you network, i.e. the initial subnetwork able to attain at least similar performances than the network as a whole.",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#quick-start",
    "href": "overview.html#quick-start",
    "title": "Overview",
    "section": "Quick Start",
    "text": "Quick Start\n\n0. Import fasterai\nfrom fasterai.sparse.all import *\n\n\n1. Create your model with fastai\nlearn = cnn_learner(dls, model)\n\n\n2. Get you Fasterai Callback\nsp_cb=SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n3. Train you model to make it sparse !\nlearn.fit_one_cycle(n_epochs, cbs=sp_cb)",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#installation",
    "href": "overview.html#installation",
    "title": "Overview",
    "section": "Installation",
    "text": "Installation\npip install git+https://github.com/nathanhubens/fasterai.git\nor\npip install fasterai",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#tutorials",
    "href": "overview.html#tutorials",
    "title": "Overview",
    "section": "Tutorials",
    "text": "Tutorials\n\nGet Started with FasterAI\nCreate your own pruning schedule\nFind winning tickets using the Lottery Ticket Hypothesis\nUse Knowledge Distillation to help a student model to reach higher performance\nSparsify Transformers\nMore to come…",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#citing",
    "href": "overview.html#citing",
    "title": "Overview",
    "section": "Citing",
    "text": "Citing\n@software{Hubens,\n  author       = {Nathan Hubens},\n  title        = {fasterai},\n  year         = 2022,\n  publisher    = {Zenodo},\n  version      = {v0.1.6},\n  doi          = {10.5281/zenodo.6469868},\n  url          = {https://doi.org/10.5281/zenodo.6469868}\n}",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#license",
    "href": "overview.html#license",
    "title": "Overview",
    "section": "License",
    "text": "License\nApache-2.0 License.",
    "crumbs": [
      "Get Started",
      "Overview"
    ]
  },
  {
    "objectID": "core.schedules.html#one-shot",
    "href": "core.schedules.html#one-shot",
    "title": "Schedules",
    "section": "One-Shot",
    "text": "One-Shot\nThe easiest schedule is the one-shot pruning, i.e. prune the network once. This can be done by simply returning the desired sparsity value. The moment when you want to prune will be controlled by the start_epoch argument in the SparsifyCallback.\n\n\nsched_oneshot\n\n sched_oneshot (start, end, pos)\n\n\none_shot.plot(50)",
    "crumbs": [
      "Get Started",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core.schedules.html#iterative",
    "href": "core.schedules.html#iterative",
    "title": "Schedules",
    "section": "Iterative",
    "text": "Iterative\nInstead of pruning the network to desired sparsity in one step, you can do it iteratively. In fasterai, you can change the amount of iterations\n\n\nsched_iterative\n\n sched_iterative (start, end, pos, n_steps=3)\n\nPerform iterative pruning, and pruning in n_steps steps\n\niterative.plot(50)\n\n\n\n\n\n\n\n\n\n\nTo modify the default n_steps, you can use the partial function.\n\niterative = Schedule(partial(sched_iterative, n_steps=5), start_pct=0.2)\n\n\niterative.plot(50)",
    "crumbs": [
      "Get Started",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core.schedules.html#automated-gradual-pruning",
    "href": "core.schedules.html#automated-gradual-pruning",
    "title": "Schedules",
    "section": "Automated Gradual Pruning",
    "text": "Automated Gradual Pruning\nSome researchers have come up with more sophisticated schedules, such as the Automated Gradual Pruning.\n\n\nsched_agp\n\n sched_agp (start, end, pos)\n\n\nagp.plot(50)",
    "crumbs": [
      "Get Started",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core.schedules.html#one-cycle-pruning",
    "href": "core.schedules.html#one-cycle-pruning",
    "title": "Schedules",
    "section": "One-Cycle Pruning",
    "text": "One-Cycle Pruning\n\n\nsched_onecycle\n\n sched_onecycle (start, end, pos, α=14, β=6)\n\n\none_cycle.plot(50)\n\n\n\n\n\n\n\n\n\n\nOn top of that, all of the schedules available in fastai by default are also available: - sched_cos - sched_linear",
    "crumbs": [
      "Get Started",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core.schedules.html#dense-sparse-dense",
    "href": "core.schedules.html#dense-sparse-dense",
    "title": "Schedules",
    "section": "Dense-Sparse-Dense",
    "text": "Dense-Sparse-Dense\nYou can also create even more interesting behaviours such as the DSD method, where you prune the model in the first place, then re-grow it to its initial amount of parameter.\n\n\nsched_dsd\n\n sched_dsd (start, end, pos)\n\n\ndsd.plot(50)",
    "crumbs": [
      "Get Started",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "distill.knowledge_distillation.html",
    "href": "distill.knowledge_distillation.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge Distillation, sometimes called teacher-student training, is a compression method in which a small (the student) model is trained to mimic the behaviour of a larger (the teacher) model.\nThe main goal is to reveal what is called the Dark Knowledge hidden in the teacher model.\nIf we take the same example provided by Geoffrey Hinton et al., we have\nThe main problem of classification is that the output activation function (softmax) will, by design, make a single value really high and squash others.\n\\[\np_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)}\n\\]\nWith \\(p_i\\) the probability of class \\(i\\), computed from the logits \\(z\\)\nHere is an example to illustrate this phenomenon:\nLet’s say that we have trained a model to discriminate between the following 5 classes: [cow, dog, plane, cat, car]\nAnd here is the output of the final layer (the logits) when the model is fed a new input image:\n\nlogits = torch.tensor([1.3, 3.1, 0.2, 1.9, -0.3])\n\nBy judging on the predictions, the model seems confident that the input data is a dog and quite confident that it is definitely not a plane nor a car, with predictions for cow and cat being moderately high.\nSo the model not only has learned to recognize a dog in the image, but also that a dog is very different from a car and a plane and share similarities with cats and cows. This information is what is called dark knowledge !\nWhen passing those predictions through a softmax, we have:\n\npredictions = F.softmax(logits, dim=-1); predictions\n\ntensor([0.1063, 0.6431, 0.0354, 0.1937, 0.0215])\n\n\nThis is accuenting the differences that we had earlier, discarding some of the dark knowledge acquired earlier. The way to keep this knowledge is to “soften” our softmax outputs, by adding a temperature parameter. The higher the temperature, the softer the predictions.\n\nsoft_predictions = F.softmax(logits/3, dim=-1); soft_predictions\n\ntensor([0.1879, 0.3423, 0.1302, 0.2294, 0.1102])\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the Temperature is equal to 1, then we have regular softmax\n\n\nWhen applying Knowledge Distillation, we want to keep the Dark Knowledge that the teacher model has acquired during its training but not rely entirely on it. So we combine two losses:\n\nThe Teacher loss between the softened predictions of the teacher and the softened predictions of the student\nThe Classification loss, which is the regular loss between hard labels and hard predictions\n\nThe combination between those losses are weighted by an additional parameter α, as:\n\\[\nL_{K D}=\\alpha  * \\text { CrossEntropy }\\left(p_{S}^{\\tau}, p_{T}^{\\tau}\\right)+(1-\\alpha) * \\text { CrossEntropy }\\left(p_{S}, y_{\\text {true }}\\right)\n\\]\nWith \\(p^{\\tau}\\) being the softened predictions of the student and teacher\n\n\n\n\n\n\nNote\n\n\n\nIn practice, the distillation loss will be a bit different in the implementation\n\n\n\nThis can be done with fastai, using the Callback system !\n\n\nKnowledgeDistillationCallback\n\n KnowledgeDistillationCallback (teacher, loss, activations_student=None,\n                                activations_teacher=None, weight=0.5)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nThe loss function that is used may depend on the use case. For classification, we usually use the one presented above, named SoftTarget in fasterai. But for regression cases, we may want to perform regression on the logits directly.",
    "crumbs": [
      "Get Started",
      "Distill",
      "Knowledge Distillation"
    ]
  },
  {
    "objectID": "tutorial.bn_folding.html",
    "href": "tutorial.bn_folding.html",
    "title": "BatchNorm Folding",
    "section": "",
    "text": "This is how to do it with fasterai !\n\nGet the data\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\nTrain the model\n\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(5)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.615641\n0.596630\n0.688092\n00:10\n\n\n1\n0.582679\n0.558671\n0.689445\n00:10\n\n\n2\n0.529308\n0.517995\n0.744926\n00:10\n\n\n3\n0.481804\n0.449941\n0.784168\n00:10\n\n\n4\n0.400030\n0.414093\n0.800406\n00:10\n\n\n\n\n\n\nFold !\n\n\nbn = BN_Folder()\nnew_model = bn.fold(learn.model)\n\nThe batch norm layers have been replaced by an Identity layer, and the weights of the convolutions have been modified accordingly.\n\nnew_model\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n  (bn1): Identity()\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\nWe can see that the new model possess fewer parameters\n\ncount_parameters(learn.model)\n\n11177538\n\n\n\ncount_parameters(new_model)\n\n11172738\n\n\nBut is also faster to run !\n\nx,y = dls.one_batch()\n\n\nlearn.model(x[0][None].cuda())\n\n5.59 ms ± 547 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nnew_model(x[0][None].cuda())\n\n4.14 ms ± 446 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nBut most importantly, has the exact same perfomance as before:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\n\n\nnew_learn.validate()\n\n\n\n\n(#2) [0.4140927791595459,0.8004059791564941]",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "core.granularity.html",
    "href": "core.granularity.html",
    "title": "Granularity",
    "section": "",
    "text": "A Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n\n\n\n\n\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')",
    "crumbs": [
      "Get Started",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core.granularity.html#conv2d-pruning",
    "href": "core.granularity.html#conv2d-pruning",
    "title": "Granularity",
    "section": "",
    "text": "A Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n\n\n\n\n\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')",
    "crumbs": [
      "Get Started",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core.granularity.html#linear-pruning",
    "href": "core.granularity.html#linear-pruning",
    "title": "Granularity",
    "section": "Linear Pruning",
    "text": "Linear Pruning\n\n0-D Blocks\nAs for the convolution filters, weights from a Linear layer can be removed independently.\n\nweight: remove individual weights.\n\n\nget_pruned_linear('weight')\n\n\n\n\n\n\n\n\n\n\n1-D Blocks\n\ncolumn: remove column of weight, which corresponds to removing input neurons.\n\n\nget_pruned_linear('column')\n\n\n\n\n\n\n\n\n\nrow: remove rows of weight, which corresponds to removing output neurons.\n\n\nget_pruned_linear('row')",
    "crumbs": [
      "Get Started",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core.granularity.html#transformer-pruning",
    "href": "core.granularity.html#transformer-pruning",
    "title": "Granularity",
    "section": "Transformer Pruning",
    "text": "Transformer Pruning\n\n\n\n\n\n\nNote\n\n\n\nThis is an experimental part of the library",
    "crumbs": [
      "Get Started",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "prune.prune_callback.html",
    "href": "prune.prune_callback.html",
    "title": "Prune Callback",
    "section": "",
    "text": "PruneCallback\n\n PruneCallback (sparsity:int, context:str, criteria:Callable,\n                schedule:Callable,\n                model:torch.nn.modules.module.Module=None,\n                round_to:int=None,\n                layer_type:torch.nn.modules.module.Module=&lt;class\n                'torch.nn.modules.conv.Conv2d'&gt;)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events",
    "crumbs": [
      "Get Started",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "quantize.quantize_callback.html",
    "href": "quantize.quantize_callback.html",
    "title": "Quantize Callback",
    "section": "",
    "text": "QuantizeCallback\n\n QuantizeCallback (qconfig_mapping=None, backend='x86')\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nimport timm\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn.model.fc = nn.Linear(512, 2)\nlearn.fit_one_cycle(10, cbs=QuantizeCallback())\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.593340\n0.501450\n0.718539\n00:12\n\n\n1\n0.395149\n0.378658\n0.830853\n00:11\n\n\n2\n0.247482\n0.231792\n0.901218\n00:11\n\n\n3\n0.164855\n0.265116\n0.892422\n00:11\n\n\n4\n0.112191\n0.228632\n0.914073\n00:11\n\n\n5\n0.070206\n0.214058\n0.929635\n00:11\n\n\n6\n0.050446\n0.202638\n0.937754\n00:11\n\n\n7\n0.033936\n0.203362\n0.941137\n00:11\n\n\n8\n0.024784\n0.201417\n0.938430\n00:11\n\n\n9\n0.022369\n0.193307\n0.941813\n00:12\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\nlearn.fit(5, cbs=QuantizeCallback())\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.571546\n0.424050\n0.789581\n00:06\n\n\n1\n0.469406\n0.363851\n0.843708\n00:06\n\n\n2\n0.407703\n0.399239\n0.817997\n00:06\n\n\n3\n0.375065\n0.309377\n0.865359\n00:06\n\n\n4\n0.323774\n0.331475\n0.873478\n00:06\n\n\n\n\n\n\nlearn.model\n\nGraphModule(\n  (0): Module(\n    (0): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.029317768290638924, zero_point=0, padding=(3, 3))\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.017887497320771217, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0466480627655983, zero_point=66, padding=(1, 1))\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.017889995127916336, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07470479607582092, zero_point=66, padding=(1, 1))\n      )\n    )\n    (5): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.0174386166036129, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.047718875110149384, zero_point=60, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.04965509846806526, zero_point=68)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.019585009664297104, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.05827442184090614, zero_point=70, padding=(1, 1))\n      )\n    )\n    (6): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.02278205193579197, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.05654977634549141, zero_point=57, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.019852932542562485, zero_point=75)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.021630365401506424, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.06945421546697617, zero_point=73, padding=(1, 1))\n      )\n    )\n    (7): Module(\n      (0): Module(\n        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.019869942218065262, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.07700460404157639, zero_point=63, padding=(1, 1))\n        (downsample): Module(\n          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.045847173780202866, zero_point=68)\n        )\n      )\n      (1): Module(\n        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.02446889691054821, zero_point=0, padding=(1, 1))\n        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.3400780260562897, zero_point=54, padding=(1, 1))\n      )\n    )\n  )\n  (1): Module(\n    (0): Module(\n      (mp): AdaptiveMaxPool2d(output_size=1)\n      (ap): AdaptiveAvgPool2d(output_size=1)\n    )\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): QuantizedDropout(p=0.25, inplace=False)\n    (4): QuantizedLinearReLU(in_features=1024, out_features=512, scale=0.5987890958786011, zero_point=0, qscheme=torch.per_channel_affine)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): QuantizedDropout(p=0.5, inplace=False)\n    (8): QuantizedLinear(in_features=512, out_features=2, scale=0.6221694350242615, zero_point=113, qscheme=torch.per_channel_affine)\n  )\n)",
    "crumbs": [
      "Get Started",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorial.sparsify_callback.html",
    "href": "tutorial.sparsify_callback.html",
    "title": "Sparsify Callback",
    "section": "",
    "text": "from fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\n\nimport timm\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\n\n\n\n\n\nlearn = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn.fc = nn.Linear(512, 2)\nsp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=one_cycle)\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of filter until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.434834\n0.498019\n0.758457\n00:07\n\n\n1\n0.409158\n0.432628\n0.809202\n00:06\n\n\n2\n0.338707\n0.371978\n0.832206\n00:07\n\n\n3\n0.274749\n0.368416\n0.854533\n00:06\n\n\n4\n0.237638\n0.373818\n0.849797\n00:06\n\n\n\n\n\nSparsity at the end of epoch 0: [1.96]%\nSparsity at the end of epoch 1: [20.07]%\nSparsity at the end of epoch 2: [45.86]%\nSparsity at the end of epoch 3: [49.74]%\nSparsity at the end of epoch 4: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 53.12%\nSparsity in Conv2d 12: 50.47%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 21: 50.53%\nSparsity in Conv2d 25: 50.00%\nSparsity in Conv2d 30: 50.41%\nSparsity in Conv2d 35: 50.00%\nSparsity in Conv2d 40: 50.24%\nSparsity in Conv2d 44: 50.00%\nSparsity in Conv2d 47: 50.00%\nSparsity in Conv2d 52: 50.24%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 61: 50.38%\nSparsity in Conv2d 65: 50.00%\nSparsity in Conv2d 70: 50.11%\nSparsity in Conv2d 75: 50.00%\nSparsity in Conv2d 80: 50.12%\nSparsity in Conv2d 84: 50.00%\nSparsity in Conv2d 87: 50.00%\nSparsity in Conv2d 92: 50.09%\nSparsity in Conv2d 96: 50.00%\nSparsity in Conv2d 101: 50.10%\nSparsity in Conv2d 105: 50.00%\nSparsity in Conv2d 110: 50.03%\nSparsity in Conv2d 114: 50.00%\nSparsity in Conv2d 119: 50.11%\nSparsity in Conv2d 123: 50.00%\nSparsity in Conv2d 128: 50.12%\nSparsity in Conv2d 133: 50.00%\nSparsity in Conv2d 138: 50.08%\nSparsity in Conv2d 142: 50.00%\nSparsity in Conv2d 145: 50.01%\nSparsity in Conv2d 150: 50.14%\nSparsity in Conv2d 154: 50.00%\nSparsity in Conv2d 159: 50.28%\n\n\n\nlearn.model.conv1.weight\n\nParameter containing:\ntensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          ...,\n          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -0.0000e+00],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00, -0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          ...,\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00]]],\n\n\n        [[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n           -0.0000e+00,  0.0000e+00],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00,  0.0000e+00],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]],\n\n         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n            0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          ...,\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00],\n          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n           -0.0000e+00, -0.0000e+00]]],\n\n\n        ...,\n\n\n        [[[-7.4296e-03, -7.5463e-03, -8.3205e-03,  ..., -7.1928e-02,\n            1.5917e-01,  1.2489e-01],\n          [ 1.0137e-01, -5.4320e-03, -7.5936e-03,  ..., -4.2560e-01,\n           -1.0976e-02,  1.9817e-01],\n          [ 1.2293e-01,  2.8018e-01,  3.2716e-01,  ..., -6.7938e-01,\n           -4.9350e-01, -1.6077e-02],\n          ...,\n          [-1.8629e-01, -1.9095e-01, -2.0495e-02,  ...,  5.9457e-01,\n           -1.7923e-02, -2.3116e-01],\n          [-1.1156e-01, -2.5112e-01, -3.7146e-01,  ...,  3.9388e-01,\n            2.8940e-01, -1.7420e-02],\n          [-9.4251e-03, -1.6436e-02, -3.3763e-01,  ..., -1.1631e-02,\n            1.7691e-01,  1.0555e-01]],\n\n         [[-8.8509e-03, -6.3883e-03, -5.0388e-03,  ...,  4.3923e-03,\n            1.0098e-01,  6.4890e-03],\n          [-7.3702e-03, -2.3618e-03, -2.8961e-03,  ..., -2.6874e-01,\n           -2.7887e-03,  1.6050e-01],\n          [-5.0071e-03,  1.6799e-01,  2.7184e-01,  ..., -5.8726e-01,\n           -3.1158e-01, -6.8676e-03],\n          ...,\n          [-1.5120e-01, -2.3115e-01, -1.0500e-01,  ...,  4.9077e-01,\n           -8.2684e-03, -1.3619e-01],\n          [-7.1860e-03, -1.8193e-01, -3.3559e-01,  ...,  2.7722e-01,\n            1.6240e-01, -9.3599e-03],\n          [-2.4150e-03, -7.1116e-03, -1.8751e-01,  ..., -1.6245e-04,\n            1.1394e-01, -8.0694e-03]],\n\n         [[-3.9283e-03, -3.3088e-03, -3.5065e-03,  ...,  3.9558e-03,\n            1.9028e-03,  3.6099e-03],\n          [-1.2535e-03,  1.3431e-03, -7.3769e-04,  ..., -2.8860e-03,\n           -3.9271e-03, -4.3433e-03],\n          [ 1.9957e-03,  6.6360e-03,  2.0233e-03,  ..., -1.9471e-01,\n           -7.5721e-03, -8.9046e-03],\n          ...,\n          [-4.8502e-03, -9.2381e-02, -7.7613e-02,  ...,  1.2395e-01,\n           -1.0181e-02, -9.2624e-03],\n          [ 1.8123e-03, -3.5798e-03, -1.1415e-01,  ..., -5.0590e-03,\n           -8.1797e-03, -9.5528e-03],\n          [ 5.2121e-03, -2.0169e-04,  1.2455e-03,  ...,  1.7431e-03,\n           -4.9568e-03, -8.3131e-03]]],\n\n\n        [[[ 7.8349e-03,  1.3473e-02,  8.7682e-02,  ...,  1.4227e-01,\n            1.4403e-02,  1.1514e-02],\n          [ 5.7151e-03,  1.5254e-01, -1.6662e-01,  ..., -1.9651e-01,\n            9.3270e-03,  9.1108e-03],\n          [ 8.3750e-02, -6.4736e-02, -7.7611e-02,  ...,  1.6225e-01,\n           -2.5000e-01,  2.0282e-01],\n          ...,\n          [-9.0023e-02,  1.0453e-01, -1.6298e-01,  ..., -1.1992e-01,\n            3.8632e-03, -1.2307e-01],\n          [ 1.1366e-02,  1.7510e-01, -2.6541e-01,  ...,  5.9963e-03,\n           -2.4098e-01,  2.6635e-01],\n          [ 9.7180e-03,  1.6214e-01, -2.6019e-01,  ..., -1.3749e-01,\n           -6.4509e-02, -1.0232e-03]],\n\n         [[ 1.8908e-02,  2.6516e-02, -8.7681e-02,  ...,  2.7610e-02,\n            2.3307e-02,  1.2432e-01],\n          [ 1.5978e-02,  1.7737e-02,  2.3648e-02,  ...,  2.1288e-02,\n            1.9111e-02, -1.5350e-01],\n          [ 1.8378e-02, -8.8506e-02,  3.1728e-01,  ..., -2.9368e-01,\n            5.7689e-01, -1.4814e-01],\n          ...,\n          [-7.0136e-02,  2.9066e-02, -6.5593e-02,  ...,  3.0752e-03,\n           -3.0135e-01,  1.3104e-01],\n          [ 1.6101e-02,  2.9628e-01, -1.2312e-01,  ...,  1.9397e-01,\n            2.5778e-01,  5.0888e-03],\n          [ 1.4030e-02, -2.0212e-01,  3.2355e-01,  ...,  7.0312e-03,\n            8.5274e-03, -1.1626e-01]],\n\n         [[ 1.8303e-02,  2.4612e-02,  2.9882e-02,  ..., -9.8176e-02,\n            2.3368e-02,  1.9999e-02],\n          [ 1.5427e-02,  1.6103e-02,  1.4962e-01,  ...,  2.1504e-01,\n            1.8854e-02,  1.6187e-01],\n          [-5.3391e-02,  1.4294e-01, -2.2367e-01,  ...,  1.3511e-02,\n           -1.8316e-01, -8.3739e-02],\n          ...,\n          [ 2.1427e-01, -1.5982e-01,  1.8060e-01,  ..., -9.3084e-02,\n            4.8867e-01, -1.2281e-01],\n          [ 9.6726e-03, -3.7949e-01,  3.9740e-01,  ..., -1.5790e-01,\n           -1.5290e-01, -1.8989e-01],\n          [ 7.3821e-03,  7.1633e-03,  2.5753e-03,  ..., -3.3214e-03,\n            1.5466e-01, -6.4393e-03]]],\n\n\n        [[[ 1.4643e-01,  5.8291e-03, -9.3539e-02,  ..., -1.0934e-01,\n           -6.7021e-02,  1.4150e-02],\n          [-1.4916e-01, -3.0628e-01, -3.8497e-01,  ..., -3.3362e-01,\n           -2.2944e-01, -1.6075e-01],\n          [-1.6706e-01, -2.8656e-01, -3.0256e-01,  ..., -2.0954e-01,\n           -1.3465e-01,  9.8437e-03],\n          ...,\n          [ 5.5858e-03,  6.4734e-03,  1.8051e-01,  ...,  3.8381e-01,\n            3.5352e-01,  2.8040e-01],\n          [ 8.6895e-03,  1.1563e-01,  2.1870e-01,  ...,  3.3523e-01,\n            2.3642e-01,  1.5989e-01],\n          [ 9.8079e-02,  1.6960e-01,  2.1411e-01,  ...,  1.8952e-01,\n            6.1990e-03,  1.1999e-02]],\n\n         [[ 1.7169e-01,  3.0308e-03, -1.3982e-01,  ..., -1.6779e-01,\n           -1.0978e-01,  7.9350e-03],\n          [-1.9304e-01, -4.0593e-01, -5.3100e-01,  ..., -4.6119e-01,\n           -3.2400e-01, -2.0214e-01],\n          [-2.8363e-01, -4.3816e-01, -4.9162e-01,  ..., -3.1524e-01,\n           -2.1846e-01, -8.9559e-02],\n          ...,\n          [ 4.0925e-03,  2.3280e-03,  2.0114e-01,  ...,  4.4946e-01,\n            4.0134e-01,  3.4598e-01],\n          [ 1.1379e-01,  1.8643e-01,  3.1593e-01,  ...,  4.7618e-01,\n            3.8119e-01,  3.1284e-01],\n          [ 1.8202e-01,  2.5852e-01,  3.0201e-01,  ...,  3.2789e-01,\n            1.8261e-01,  1.0490e-01]],\n\n         [[ 1.4723e-01, -1.3924e-02, -1.1114e-02,  ..., -3.3590e-03,\n           -3.8765e-03, -9.1093e-04],\n          [-1.7495e-02, -1.6472e-01, -2.6806e-01,  ..., -2.4088e-01,\n           -1.9225e-01, -1.3964e-01],\n          [-1.1816e-01, -2.2255e-01, -2.8439e-01,  ..., -1.7405e-01,\n           -1.1457e-01, -9.2854e-03],\n          ...,\n          [-1.5639e-02, -1.7753e-02, -2.0895e-02,  ...,  2.2529e-01,\n            1.7778e-01,  1.4106e-01],\n          [-1.3705e-02, -1.7075e-02,  9.4111e-02,  ...,  2.2573e-01,\n            1.6869e-01,  1.2757e-01],\n          [ 1.0739e-01,  1.3187e-01,  1.1567e-01,  ...,  1.6808e-01,\n            1.1086e-01,  8.2069e-02]]]], device='cuda:0', requires_grad=True)\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.711885\n1.064277\n0.843708\n00:45\n\n\n1\n0.409735\n0.217008\n0.913396\n00:03\n\n\n2\n0.265280\n0.284833\n0.898512\n00:03\n\n\n3\n0.144334\n0.158726\n0.936401\n00:03\n\n\n4\n0.082726\n0.153889\n0.939784\n00:03\n\n\n\n\n\nLet’s now try adding some sparsity in our model\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_cycle)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.711270\n0.742762\n0.775372\n00:07\n\n\n1\n0.383374\n0.307700\n0.864005\n00:07\n\n\n2\n0.219235\n0.217708\n0.905954\n00:07\n\n\n3\n0.121921\n0.213659\n0.933018\n00:07\n\n\n4\n0.067208\n0.200506\n0.930988\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [1.96]%\nSparsity at the end of epoch 1: [20.07]%\nSparsity at the end of epoch 2: [45.86]%\nSparsity at the end of epoch 3: [49.74]%\nSparsity at the end of epoch 4: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%\n\n\nSurprisingly, our network that is composed of \\(50 \\%\\) of zeroes performs reasonnably well when compared to our plain and dense network.\nThe SparsifyCallback also accepts a list of sparsities, corresponding to each layer of layer_type to be pruned. Below, we show how to prune only the intermediate layers of ResNet-18.\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsparsities = [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]\n\n\nsp_cb = SparsifyCallback(sparsity=sparsities, granularity='weight', context='local', criteria=large_final, schedule=cos)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.731650\n0.570400\n0.811908\n00:07\n\n\n1\n0.396108\n0.262083\n0.895805\n00:07\n\n\n2\n0.250992\n0.210679\n0.909337\n00:07\n\n\n3\n0.132799\n0.192091\n0.925575\n00:07\n\n\n4\n0.079732\n0.159255\n0.938430\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 4: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nFinal Sparsity: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity in Conv2d 2: 0.00%\nSparsity in Conv2d 8: 0.00%\nSparsity in Conv2d 11: 0.00%\nSparsity in Conv2d 14: 0.00%\nSparsity in Conv2d 17: 0.00%\nSparsity in Conv2d 21: 0.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 0.00%\nSparsity in Conv2d 53: 0.00%\nSparsity in Conv2d 56: 0.00%\nSparsity in Conv2d 59: 0.00%\nSparsity in Conv2d 62: 0.00%\nSparsity in Conv2d 65: 0.00%\n\n\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`\n\nFor example, we correctly pruned the convolution layers of our model, but we could imagine pruning the Linear Layers of even only the BatchNorm ones !",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorial.regularizer.html",
    "href": "tutorial.regularizer.html",
    "title": "Regularize Callback",
    "section": "",
    "text": "from fasterai.core.criteria import *\nfrom fasterai.regularize.all import *\nfrom fastai.vision.all import *\n\nGet your data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nTrain a model without Regularization as a baseline\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.675158\n0.390553\n0.845061\n00:06\n\n\n1\n0.334588\n0.219738\n0.901894\n00:03\n\n\n2\n0.178833\n0.194565\n0.919486\n00:04\n\n\n\n\n\nCreate the RegularizeCallback\n\nreg_cb = RegularizeCallback('filter', wd=0.0001)\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(3, cbs=reg_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n16.783606\n15.932514\n0.855210\n00:04\n\n\n1\n14.619355\n13.426625\n0.922192\n00:03\n\n\n2\n13.077019\n12.727697\n0.921516\n00:04",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html",
    "href": "tutorial.walkthrough.html",
    "title": "Walkthrough",
    "section": "",
    "text": "size, bs = 128, 32\ndls = get_dls(size, bs)\nLet’s start with a bit of context for the purpose of the demonstration. Imagine that we want to deploy a VGG16 model on a mobile device that has limited storage capacity and that our task requires our model to run sufficiently fast. It is known that parameters and speed efficiency are not the strong points of VGG16 but let’s see what we can do with it.\nLet’s first check the number of parameters and the inference time of VGG16.\nlearn = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nnum_parameters = get_num_parameters(learn.model)\ndisk_size = get_model_size(learn.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 537.30 MB (disk), 134309962 parameters\nSo our model has 134 millions parameters and needs 537MB of disk space in order to be stored\nmodel = learn.model.eval().to('cpu')\nx,y = dls.one_batch()\nprint(f'Inference Speed: {evaluate_cpu_speed(learn.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 26.33ms\nAnd it takes 26ms to perform inference on a single image.\nSnap ! This is more than we can afford for deployment, ideally we would like our model to take only half of that…but should we give up ? Nope, there are actually a lot of techniques that we can use to help reducing the size and improve the speed of our models! Let’s see how to apply them with FasterAI.\nWe will first train our VGG16 model to have a baseline of what performance we should expect from it.\nlearn.fit_one_cycle(10, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.118902\n1.796707\n0.382930\n00:24\n\n\n1\n1.697520\n1.757372\n0.452484\n00:24\n\n\n2\n1.365785\n1.360465\n0.575287\n00:24\n\n\n3\n1.244159\n1.231451\n0.584459\n00:24\n\n\n4\n1.072958\n1.381251\n0.578599\n00:24\n\n\n5\n0.977561\n0.853106\n0.730446\n00:24\n\n\n6\n0.800931\n0.774717\n0.750573\n00:24\n\n\n7\n0.690902\n0.650088\n0.796433\n00:24\n\n\n8\n0.614583\n0.613500\n0.801019\n00:24\n\n\n9\n0.600255\n0.599785\n0.808408\n00:24\nSo we would like our network to have comparable accuracy but fewer parameters and running faster… And the first technique that we will show how to use is called Knowledge Distillation",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#knowledge-distillation",
    "href": "tutorial.walkthrough.html#knowledge-distillation",
    "title": "Walkthrough",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\nKnowledge distillation is a simple yet very efficient way to train a model. It was introduced in 2006 by Caruana et al.. The main idea behind is to use a small model (called the student) to approximate the function learned by a larger and high-performing model (called the teacher). This can be done by using the large model to pseudo-label the data. This idea has been used very recently to break the state-of-the-art accuracy on ImageNet.\nWhen we train our model for classification, we usually use a softmax as last layer. This softmax has the particularity to squish low value logits towards 0, and the highest logit towards 1. This has for effect to completely lose all the inter-class information, or what is sometimes called the dark knowledge. This is the information that is valuable and that we want to transfer from the teacher to the student.\nTo do so, we still use a regular classification loss but at the same time, we’ll use another loss, computed between the softened logits of the teacher (our soft labels) and the softened logits of the student (our soft predictions). Those soft values are obtained when you use a soft-softmax, that avoids squishing the values at its output. Our implementation follows this paper and the basic principle of training is represented in the figure below:\n\n\n\nTo use Knowledge Distillation with FasterAI, you only need to use this callback when training your student model:\n\n\n KnowledgeDistillation(teacher.model, loss) \n\n You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation. \n\n\n\n\nfrom fasterai.distill.all import *\n\nThe first thing to do is to find a teacher, which can be any model, that preferrably performs well. We will chose VGG19 for our demonstration. To make sure it performs better than our VGG16 model, let’s start from a pretrained version.\n\nteacher = vision_learner(dls, models.vgg19_bn, metrics=[accuracy])\nteacher.fit_one_cycle(3, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.921188\n0.367872\n0.891210\n00:16\n\n\n1\n0.449437\n0.222757\n0.928153\n00:16\n\n\n2\n0.426467\n0.205059\n0.935796\n00:16\n\n\n\n\n\nOur teacher has 94% of accuracy which is pretty good, it is ready to take a student under its wing. So let’s create our student model and train it with the Knowledge Distillation callback:\n\nstudent = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nkd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-4, cbs=kd_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n5.904119\n5.542538\n0.427006\n00:38\n\n\n1\n4.528544\n4.491541\n0.508790\n00:38\n\n\n2\n3.670337\n4.807127\n0.497834\n00:38\n\n\n3\n3.159431\n3.037996\n0.663694\n00:38\n\n\n4\n2.864913\n2.860051\n0.704459\n00:38\n\n\n5\n2.388322\n2.319992\n0.742420\n00:38\n\n\n6\n2.054017\n2.105503\n0.774267\n00:38\n\n\n7\n1.790077\n1.804504\n0.792611\n00:38\n\n\n8\n1.707000\n1.659822\n0.810955\n00:38\n\n\n9\n1.620380\n1.658448\n0.810446\n00:38\n\n\n\n\n\nAnd we can see that indeed, the knowledge of the teacher was useful for the student, as it is clearly overperforming the vanilla VGG16.\nOk, so now we are able to get more from a given model which is kind of cool ! With some experimentations we could come up with a model smaller than VGG16 but able to reach the same performance as our baseline! You can try to find it by yourself later, but for now let’s continue with the next technique !",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#sparsifying",
    "href": "tutorial.walkthrough.html#sparsifying",
    "title": "Walkthrough",
    "section": "Sparsifying",
    "text": "Sparsifying\nNow that we have a student model that is performing better than our baseline, we have some room to compress it. And we’ll start by making the network sparse. As explained in a previous article, there are many ways leading to a sparse network.\n\n\n\n\n\n\n\nNote\n\n\n\nUsually, the process of making a network sparse is called Pruning. I prefer using the term Pruning when parameters are actually removed from the network, which we will do in the next section.\n\n\n\n\n\nBy default, FasterAI uses the Automated Gradual Pruning paradigm as it removes parameters as the model trains and doesn’t require to pretrain the model, so it is usually much faster. In FasterAI, this is also managed by using a callback, that will replace the least important parameters of your model by zeroes during the training. The callback has a wide variety of parameters to tune your Sparsifying operation, let’s take a look at them:\n\n\nSparsifyCallback(learn, sparsity, granularity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ngranularity: on what granularity you want the sparsification to be operated\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e linear, cosine, … and gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\n\nBut let’s come back to our example!\nHere, we will make our network 40% sparse, and remove entire filters, selected locally and based on L1 norm. We will train with a learning rate a bit smaller to be gentle with our network because it has already been trained. The scheduling selected is cosinusoidal, so the pruning starts and ends quite slowly.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=cos)\nstudent.fit(10, 1e-5, cbs=sp_cb)\n\nPruning of filter until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.580802\n0.602479\n0.808917\n00:24\n\n\n1\n0.608323\n0.592913\n0.807643\n00:24\n\n\n2\n0.575951\n0.594700\n0.811210\n00:24\n\n\n3\n0.569944\n0.589884\n0.815541\n00:24\n\n\n4\n0.589329\n0.572137\n0.815796\n00:24\n\n\n5\n0.586437\n0.579539\n0.813503\n00:24\n\n\n6\n0.614333\n0.627116\n0.799236\n00:24\n\n\n7\n0.628266\n0.629826\n0.796943\n00:24\n\n\n8\n0.673765\n0.646662\n0.785478\n00:24\n\n\n9\n0.645206\n0.589828\n0.809172\n00:24\n\n\n\n\n\nSparsity at the end of epoch 0: [1.22]%\nSparsity at the end of epoch 1: [4.77]%\nSparsity at the end of epoch 2: [10.31]%\nSparsity at the end of epoch 3: [17.27]%\nSparsity at the end of epoch 4: [25.0]%\nSparsity at the end of epoch 5: [32.73]%\nSparsity at the end of epoch 6: [39.69]%\nSparsity at the end of epoch 7: [45.23]%\nSparsity at the end of epoch 8: [48.78]%\nSparsity at the end of epoch 9: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 0.00%\nSparsity in Conv2d 5: 0.00%\nSparsity in Conv2d 9: 0.00%\nSparsity in Conv2d 12: 0.00%\nSparsity in Conv2d 16: 0.00%\nSparsity in Conv2d 19: 0.00%\nSparsity in Conv2d 22: 0.00%\nSparsity in Conv2d 26: 62.30%\nSparsity in Conv2d 29: 70.51%\nSparsity in Conv2d 32: 72.07%\nSparsity in Conv2d 36: 71.29%\nSparsity in Conv2d 39: 69.92%\nSparsity in Conv2d 42: 66.41%\n\n\nOur network now has 50% of its filters composed entirely of zeroes, without even losing accuracy. Obviously, choosing a higher sparsity makes it more difficult for the network to keep a similar accuracy. Other parameters can also widely change the behaviour of our sparsification process. For example choosing a more fine-grained sparsity usually leads to better results but is then more difficult to take advantage of in terms of speed.\n\nLet’s now see how much we gained in terms of speed. Because we removed 50% of convolution filters, we should expect crazy speed-up right ?\n\nprint(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 26.93ms\n\n\nWell actually, no. We didn’t remove any parameters, we just replaced some by zeroes, remember? The amount of parameters is still the same:\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 537.31 MB (disk), 134309962 parameters\n\n\nWhich leads us to the next section.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#pruning",
    "href": "tutorial.walkthrough.html#pruning",
    "title": "Walkthrough",
    "section": "Pruning",
    "text": "Pruning\nWhy don’t we see any acceleration even though we removed half of the parameters? That’s because natively, our GPU does not know that our matrices are sparse and thus isn’t able to accelerate the computation. The easiest work around, is to physically remove the parameters we zeroed-out. But this operation requires to change the architecture of the network.\nThis pruning only works if we remove entire filters as it is the only case where we can change the architecture accordingly. Hopefully, sparse computations will soon be available on common deep learning librairies so this section will become useless in the future.\n\nHere is what it looks like with fasterai: \n\n\n\nPruneCallback(learn, sparsity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e linear, cosine, … and gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\nSo in the case of our example, it gives:\n\nfrom fasterai.prune.all import *\n\nLet’s now see what our model is capable of now:\n\npr_cb = PruneCallback(sparsity=50, context='global', criteria=large_final, schedule=cos, layer_type=[nn.Conv2d])\nstudent.fit(5, 1e-5, cbs=pr_cb)\n\nPruning until a sparsity of [50]%\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.575356\n0.585162\n0.817070\n01:21\n\n\n1\n0.596313\n0.580011\n0.815541\n00:57\n\n\n2\n0.572922\n0.581003\n0.815287\n00:46\n\n\n3\n0.565360\n0.581362\n0.815032\n00:44\n\n\n4\n0.573613\n0.574642\n0.817834\n00:44\n\n\n\n\n\nSparsity at the end of epoch 0: [4.77]%\nSparsity at the end of epoch 1: [17.27]%\nSparsity at the end of epoch 2: [32.73]%\nSparsity at the end of epoch 3: [45.23]%\nSparsity at the end of epoch 4: [50.0]%\nFinal Sparsity: [50.0]%\n\n\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 218.53 MB (disk), 54620757 parameters\n\n\nAnd in terms of speed:\n\nprint(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 16.41ms\n\n\nYay ! Now we can talk ! Let’s just double check that our accuracy is unchanged and that we didn’t mess up somewhere:\n\nAnd there is actually more that we can do ! Let’s keep going !",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#batch-normalization-folding",
    "href": "tutorial.walkthrough.html#batch-normalization-folding",
    "title": "Walkthrough",
    "section": "Batch Normalization Folding",
    "text": "Batch Normalization Folding\nBatch Normalization Folding is a really easy to implement and straightforward idea. The gist is that batch normalization is nothing more than a normalization of the input data at each layer. Moreover, at inference time, the batch statistics used for this normalization are fixed. We can thus incorporate the normalization process directly in the convolution by changing its weights and completely remove the batch normalization layers, which is a gain both in terms of parameters and in terms of computations. For a more in-depth explaination, see this blog post.\nThis is how to use it with FasterAI:\n\nbn_folder = BN_Folder()\nbn_folder.fold(learn.model))\n\n Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you don’t need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the forward method of your network. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should also be lossless as it redefines the convolution to take batch norm into account and is thus equivalent.\n\n\n\n\nfrom fasterai.misc.bn_folding import *\n\nLet’s do this with our model !\n\nbn_f = BN_Folder()\nfolded_model = bn_f.fold(student.model)\n\nThe parameters drop is generally not that significant, especially in a network such as VGG where almost all parameters are contained in the FC layers but, hey, any gain is good to take.\n\nnum_parameters = get_num_parameters(folded_model)\ndisk_size = get_model_size(folded_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 218.48 MB (disk), 54616533 parameters\n\n\nNow that we removed the batch normalization layers, we should again see a speedup.\n\nprint(f'Inference Speed: {evaluate_cpu_speed(folded_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 15.63ms\n\n\nAgain, let’s double check that we didn’t mess up somewhere:\n\nfolded_learner = Learner(dls, folded_model, metrics=[accuracy])\nfolded_learner.validate()\n\n\n\n\n\n\n\n\n(#2) [0.5746386647224426,0.8175796270370483]\n\n\nAnd we’re still not done yet ! As we know for VGG16, most of the parameters are comprised in the fully-connected layers so there should be something that we can do about it, right ?",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#fc-layers-factorization",
    "href": "tutorial.walkthrough.html#fc-layers-factorization",
    "title": "Walkthrough",
    "section": "FC Layers Factorization",
    "text": "FC Layers Factorization\nWe can indeed, factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\). With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\nIn FasterAI, to decompose the fully-connected layers of your model, here is what you need to do: \n\nFCD = FCDecomposer()\ndecomposed_model = FCD.decompose(model, percent_removed)\n\n The percent_removed corresponds to the percentage of singular values removed (k value above). \n\n\n\nget_model_size(decomposed_model)\n\n182922786\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis time, the decomposition is not exact, so we expect a drop in performance afterwards and further retraining will be needed.\n\n\nWhich gives with our example, if we only want to keep half of them:\n\nfrom fasterai.misc.fc_decomposer import *\n\n\nfc_decomposer = FC_Decomposer()\ndecomposed_model = fc_decomposer.decompose(folded_learner.model, percent_removed=0.5)\n\nHow many parameters do we have now ?\n\nnum_parameters = get_num_parameters(decomposed_model)\ndisk_size = get_model_size(decomposed_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 182.91 MB (disk), 45724167 parameters\n\n\nAnd how much time did we gain ?\n\nprint(f'Inference Speed: {evaluate_cpu_speed(decomposed_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 14.90ms\n\n\nWe actually get a network that is a little bit slower, but at the expense of reducing the by 10M the number of parameter. This is thus a matter of compromise between network weight and speed.\n\nHowever, this technique is an approximation so it is not lossless, so we should retrain our network a bit to recover its performance.\n\nfinal_learner = Learner(dls, decomposed_model, metrics=[accuracy])\nfinal_learner.fit_one_cycle(5, 1e-5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.738704\n0.744383\n0.756178\n00:15\n\n\n1\n0.711042\n0.769284\n0.752102\n00:15\n\n\n2\n0.695116\n0.728815\n0.755159\n00:15\n\n\n3\n0.653428\n0.732841\n0.752357\n00:15\n\n\n4\n0.609917\n0.702751\n0.762293\n00:15\n\n\n\n\n\nThis operation is usually less useful for more recent architectures as they usually do not have that many parameters in their fully-connected layers.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#quantization",
    "href": "tutorial.walkthrough.html#quantization",
    "title": "Walkthrough",
    "section": "Quantization",
    "text": "Quantization\n\nfrom fasterai.quantize.quantize_callback import *\n\nNow that we have removed every superfluous parameter that we could, we can still continue to compress our model. A common way to do so is now to reduce the precision of each parameter in the network, making it considerably smaller. Such an approach is called Quantization and won’t affect the total number of parameter but will make each one of them smaller to store, on top of making computations faster.\nIn FasterAI, quantization can be done in a static way, i.e. apply quantization to the model, also called Post-Training Quantization. It also can be applied dynamically during the training, also called Quantization-Aware Training.\n\nfinal_learner.fit_one_cycle(5, 1e-5, cbs=QuantizeCallback())\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.672724\n0.763105\n0.742930\n00:21\n\n\n1\n0.673187\n0.831859\n0.739108\n00:21\n\n\n2\n0.649460\n0.715663\n0.762038\n00:21\n\n\n3\n0.631277\n0.690792\n0.779873\n00:21\n\n\n4\n0.557573\n0.682759\n0.796943\n00:21\n\n\n\n\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_learner.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 11.33ms\n\n\n\nnum_parameters = count_parameters_quantized(final_learner.model)\ndisk_size = get_model_size(final_learner.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 46.02 MB (disk), 45,724,167 parameters",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.walkthrough.html#extra-acceleration",
    "href": "tutorial.walkthrough.html#extra-acceleration",
    "title": "Walkthrough",
    "section": "Extra Acceleration",
    "text": "Extra Acceleration\n\nfrom fasterai.misc.cpu_optimizer import accelerate_model_for_cpu\n\n\nfinal_model = accelerate_model_for_cpu(final_learner.model, x[0][None])\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 9.97ms\n\n\n\nnum_parameters = get_num_parameters(final_model)\ndisk_size = get_model_size(final_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 46.02 MB (disk), 0 parameters\n\n\n\nSo to recap, we saw in this article how to use fasterai to:  1. Make a student model learn from a teacher model (Knowledge Distillation)  2. Make our network sparse (Sparsifying)  3. Optionnaly physically remove the zero-filters (Pruning)  4. Remove the batch norm layers (Batch Normalization Folding)  5. Approximate our big fully-connected layers by smaller ones (Fully-Connected Layers Factorization)  6. Quantize the model to reduce the precision of the weights (Quantization)  7. Extra acceleration techniques to further optimize the speed of our network\n\nAnd we saw that by applying those, we could reduce our VGG16 model from 537 MB of parameters down to 46 MB (11x compression), and also speed-up the inference from 26.3ms to 9.9ms (2.6x speed-up) without any drop in accuracy compared to the baseline.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease keep in mind that the techniques presented above are not magic 🧙‍♂️, so do not expect to see a 200% speedup and compression everytime. What you can achieve highly depend on the architecture that you are using (some are already speed/parameter efficient by design) or the task it is doing (some datasets are so easy that you can remove almost all your network without seeing a drop in performance)",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorial.schedules.html",
    "href": "tutorial.schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "Neural Network Pruning usually follows one of the next 3 schedules:\nIn fasterai, all those 3 schedules can be applied from the same callback. We’ll cover each below\nIn the SparsifyCallback, there are several parameters to ‘shape’ our pruning schedule: * start_sparsity: the initial sparsity of our model, generally kept at 0 as after initialization, our weights are generally non-zero. * end_sparsity: the target sparsity at the end of the training * start_epoch: we can decide to start pruning right from the beginning or let it train a bit before removing weights. * sched_func: this is where the general shape of the schedule is specified as it specifies how the sparsity evolves along the training. You can either use a schedule available in fastai our even coming with your own !\npath = untar_data(URLs.PETS)\n\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\nWe will first train a network without any pruning, which will serve as a baseline.\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(6)\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.703338\n0.584460\n0.833559\n00:08\n\n\n1\n0.409266\n0.277945\n0.886333\n00:07\n\n\n2\n0.272164\n0.230810\n0.900541\n00:07\n\n\n3\n0.171919\n0.202396\n0.920162\n00:07\n\n\n4\n0.095116\n0.168309\n0.937754\n00:07\n\n\n5\n0.055808\n0.167708\n0.939107\n00:07",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorial.schedules.html#one-shot-pruning",
    "href": "tutorial.schedules.html#one-shot-pruning",
    "title": "Schedules",
    "section": "One-Shot Pruning",
    "text": "One-Shot Pruning\nThe simplest way to perform pruning is called One-Shot Pruning. It consists of the following three steps:\n\nYou first need to train a network\nYou then need to remove some weights (depending on your criteria, needs,…)\nYou fine-tune the remaining weights to recover from the loss of parameters.\n\nWith fasterai, this is really easy to do. Let’s illustrate it by an example:\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning. This training can be done independently from the pruning callback, or simulated by the start_epoch that will delay the pruning process.\nYou thus only need to create the Callback with the one_shot schedule and set the start_epoch argument, i.e. how many epochs you want to train your network before pruning it.\n\nsp_cb=SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_shot)\n\nLet’s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit_one_cycle(6, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.671942\n0.521863\n0.809878\n00:07\n\n\n1\n0.433165\n0.335386\n0.868742\n00:07\n\n\n2\n0.252873\n0.223187\n0.906631\n00:07\n\n\n3\n0.151653\n0.195924\n0.922869\n00:07\n\n\n4\n0.091317\n0.169764\n0.929635\n00:07\n\n\n5\n0.055428\n0.161210\n0.934371\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [50.0]%\nSparsity at the end of epoch 3: [50.0]%\nSparsity at the end of epoch 4: [50.0]%\nSparsity at the end of epoch 5: [50.0]%\nFinal Sparsity: [50]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorial.schedules.html#iterative-pruning",
    "href": "tutorial.schedules.html#iterative-pruning",
    "title": "Schedules",
    "section": "Iterative Pruning",
    "text": "Iterative Pruning\nResearchers have come up with a better way to do pruning than pruning all the weigths in once (as in One-Shot Pruning). The idea is to perform several iterations of pruning and fine-tuning and is thus called Iterative Pruning.\n\nYou first need to train a network\nYou then need to remove a part of the weights weights (depending on your criteria, needs,…)\nYou fine-tune the remaining weights to recover from the loss of parameters.\nBack to step 2.\n\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning.\nYou only need to create the Callback with the iterative schedule and set the start_epoch argument, i.e. how many epochs you want to train your network before pruning it.\nThe iterative schedules has a n_stepsparameter, i.e. how many iterations of pruning/fine-tuning you want to perform. To modify its value, we can use the partial function like this:\niterative = partial(iterative, n_steps=5)\n\nsp_cb=SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=iterative)\n\nLet’s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit_one_cycle(6, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.747520\n0.534082\n0.823410\n00:07\n\n\n1\n0.406695\n0.292009\n0.876184\n00:07\n\n\n2\n0.248393\n0.211495\n0.912720\n00:07\n\n\n3\n0.138455\n0.226409\n0.912720\n00:07\n\n\n4\n0.092650\n0.205549\n0.926928\n00:07\n\n\n5\n0.056216\n0.195934\n0.931664\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [16.67]%\nSparsity at the end of epoch 2: [33.33]%\nSparsity at the end of epoch 3: [33.33]%\nSparsity at the end of epoch 4: [50.0]%\nSparsity at the end of epoch 5: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorial.schedules.html#gradual-pruning",
    "href": "tutorial.schedules.html#gradual-pruning",
    "title": "Schedules",
    "section": "Gradual Pruning",
    "text": "Gradual Pruning\nHere is for example how to implement the Automated Gradual Pruning schedule.\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsp_cb=SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=agp)\n\nLet’s start pruning after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit_one_cycle(6, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.675533\n0.671494\n0.815291\n00:07\n\n\n1\n0.414929\n0.261510\n0.888363\n00:07\n\n\n2\n0.261279\n0.247027\n0.903924\n00:07\n\n\n3\n0.151988\n0.198519\n0.914750\n00:07\n\n\n4\n0.088916\n0.157761\n0.933694\n00:07\n\n\n5\n0.043516\n0.148362\n0.940460\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [21.02]%\nSparsity at the end of epoch 2: [37.79]%\nSparsity at the end of epoch 3: [46.39]%\nSparsity at the end of epoch 4: [49.55]%\nSparsity at the end of epoch 5: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%\n\n\nEven though they are often considered as different pruning methods, those 3 schedules can be captured by the same Callback. Here is how the sparsity in the network evolves for those methods;\nLet’s take an example here. Let’s say that we want to train our network for 3 epochs without pruning and then 7 epochs with pruning.\nThen this is what our different pruning schedules will look like:\n\n\n\n\n\n\n\n\n\nYou can also come up with your own pruning schedule !",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "misc.bn_folding.html",
    "href": "misc.bn_folding.html",
    "title": "Batch Norm Folding",
    "section": "",
    "text": "Batch Normalization is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.\nIt consists of 2 steps:\n\nNormalize the batch by first subtracting its mean \\(\\mu\\), then dividing it by its standard deviation \\(\\sigma\\).\nFurther scale by a factor \\(\\gamma\\) and shift by a factor \\(\\beta\\). Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\\[\n\\begin{aligned}\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned}\\]\nDue to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\nOnce the training has ended, each batch normalization layer possesses a specific set of \\(\\gamma\\) and \\(\\beta\\), but also \\(\\mu\\) and \\(\\sigma\\), the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\nAs a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\nThis would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\nWith a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\nAs a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input \\(x\\), as:\n\\[\\begin{aligned} z &=W * x+b \\\\ \\mathrm{out} &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nSo, if we re-arrange the \\(W\\) and \\(b\\) of the convolution to take the parameters of the batch normalization into account, as such:\n\\[\\begin{aligned} w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\ b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nIn practice, this can be achieved in FasterAI with the BN_folder class\n\n\nBN_Folder.fold\n\n BN_Folder.fold (model)\n\nA tutorial about how to use the BN_Folder functionalities can be found here",
    "crumbs": [
      "Get Started",
      "Misc",
      "Batch Norm Folding"
    ]
  },
  {
    "objectID": "misc.cpu_optimizer.html",
    "href": "misc.cpu_optimizer.html",
    "title": "Further optimize for CPU inference",
    "section": "",
    "text": "accelerate_model_for_cpu\n\n accelerate_model_for_cpu (model:torch.nn.modules.module.Module,\n                           example_input:torch.Tensor)"
  },
  {
    "objectID": "misc.fc_decomposer.html",
    "href": "misc.fc_decomposer.html",
    "title": "Fully-Connected Layers Decomposer",
    "section": "",
    "text": "We can factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\) With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\n\n\nFC_Decomposer.decompose\n\n FC_Decomposer.decompose (model, percent_removed=0.5)\n\nA tutorial about how to use the FC_Decomposer functionalities can be found here",
    "crumbs": [
      "Get Started",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "tutorial.sparsifier.html",
    "href": "tutorial.sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network’s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nLet’s start by creating a model\nmodel = resnet18()\nAs you probably know, weights in a convolutional neural network have 4 dimensions ($ c_{out} c_{in} k_h k_w$)\nmodel.conv1.weight.ndim\n\n4\nIn the case of ResNet18, the dimension of the first layer weights is \\(64 \\times 3 \\times 7 \\times 7\\). We thus can plot each of the \\(64\\) filter as a \\(7 \\times 7\\) color image (because they contains \\(3\\) channels).\nplot_kernels(model.conv1)\nThe Sparsifier class allows us to remove some (part of) the filters, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\nUser can pass a single layer to prune by using the Sparsifier.sparsify_layer method.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorial.sparsifier.html#granularity",
    "href": "tutorial.sparsifier.html#granularity",
    "title": "Sparsifier",
    "section": "Granularity",
    "text": "Granularity\nAs we said earlier, the granularity defines the structure of parameter that you will remove.\nIn the example below, we removed weight from each convolutional filter, meaning that we now have sparse filters, as can be seen in the image below:\n\nplot_kernels(model.conv1)\n\n\n\n\n\n\n\n\nAnother granularity is, for example, removing column vectors from the filters. To do so, just change the granularity parameter accordingly.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'column', 'local', large_final)\nsparsifier.sparsify_layer(model.conv1, 70)\n\n\nplot_kernels(model.conv1)\n\n\n\n\n\n\n\n\nFor more information and examples about the pruning granularities, I suggest you to take a look at the corresponding section.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorial.sparsifier.html#context",
    "href": "tutorial.sparsifier.html#context",
    "title": "Sparsifier",
    "section": "Context",
    "text": "Context\nThe context defines where to look in the model, i.e. from where do we compare weight. The two basic contexts are: * local, i.e. we compare weight from each layer individually. This will lead to layers with similar levels of sparsity. * global, i.e. we compare weight from the whole model. This will lead to layers with different levels of sparsity\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'local', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\nSparsity in Conv2d 1: 69.99%\nSparsity in Conv2d 7: 70.00%\nSparsity in Conv2d 10: 70.00%\nSparsity in Conv2d 13: 70.00%\nSparsity in Conv2d 16: 70.00%\nSparsity in Conv2d 20: 70.00%\nSparsity in Conv2d 23: 70.00%\nSparsity in Conv2d 26: 70.00%\nSparsity in Conv2d 29: 70.00%\nSparsity in Conv2d 32: 70.00%\nSparsity in Conv2d 36: 70.00%\nSparsity in Conv2d 39: 70.00%\nSparsity in Conv2d 42: 70.00%\nSparsity in Conv2d 45: 70.00%\nSparsity in Conv2d 48: 70.00%\nSparsity in Conv2d 52: 70.00%\nSparsity in Conv2d 55: 70.00%\nSparsity in Conv2d 58: 70.00%\nSparsity in Conv2d 61: 70.00%\nSparsity in Conv2d 64: 70.00%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\nSparsity in Conv2d 1: 66.14%\nSparsity in Conv2d 7: 32.17%\nSparsity in Conv2d 10: 32.31%\nSparsity in Conv2d 13: 32.28%\nSparsity in Conv2d 16: 31.73%\nSparsity in Conv2d 20: 44.03%\nSparsity in Conv2d 23: 44.44%\nSparsity in Conv2d 26: 15.32%\nSparsity in Conv2d 29: 44.43%\nSparsity in Conv2d 32: 44.24%\nSparsity in Conv2d 36: 59.17%\nSparsity in Conv2d 39: 59.22%\nSparsity in Conv2d 42: 22.02%\nSparsity in Conv2d 45: 59.24%\nSparsity in Conv2d 48: 59.14%\nSparsity in Conv2d 52: 75.82%\nSparsity in Conv2d 55: 75.86%\nSparsity in Conv2d 58: 30.28%\nSparsity in Conv2d 61: 75.86%\nSparsity in Conv2d 64: 75.88%",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorial.sparsifier.html#criteria",
    "href": "tutorial.sparsifier.html#criteria",
    "title": "Sparsifier",
    "section": "Criteria",
    "text": "Criteria\nThe criteria defines how we select the parameters to remove. It is usually given by a scoring method. The most common one is the large_final, i.e. select parameters with the highest absolute value as they are supposed to contribute the most to the final results of the model.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\nSparsity in Conv2d 1: 66.46%\nSparsity in Conv2d 7: 31.56%\nSparsity in Conv2d 10: 31.81%\nSparsity in Conv2d 13: 31.92%\nSparsity in Conv2d 16: 32.47%\nSparsity in Conv2d 20: 44.29%\nSparsity in Conv2d 23: 43.94%\nSparsity in Conv2d 26: 15.31%\nSparsity in Conv2d 29: 44.11%\nSparsity in Conv2d 32: 44.10%\nSparsity in Conv2d 36: 59.13%\nSparsity in Conv2d 39: 59.23%\nSparsity in Conv2d 42: 21.60%\nSparsity in Conv2d 45: 59.36%\nSparsity in Conv2d 48: 59.32%\nSparsity in Conv2d 52: 75.91%\nSparsity in Conv2d 55: 75.88%\nSparsity in Conv2d 58: 30.19%\nSparsity in Conv2d 61: 75.80%\nSparsity in Conv2d 64: 75.87%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', small_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\nSparsity in Conv2d 1: 38.87%\nSparsity in Conv2d 7: 2.38%\nSparsity in Conv2d 10: 0.65%\nSparsity in Conv2d 13: 1.63%\nSparsity in Conv2d 16: 1.32%\nSparsity in Conv2d 20: 1.72%\nSparsity in Conv2d 23: 4.12%\nSparsity in Conv2d 26: 0.17%\nSparsity in Conv2d 29: 0.97%\nSparsity in Conv2d 32: 7.12%\nSparsity in Conv2d 36: 26.24%\nSparsity in Conv2d 39: 8.62%\nSparsity in Conv2d 42: 0.22%\nSparsity in Conv2d 45: 10.87%\nSparsity in Conv2d 48: 4.43%\nSparsity in Conv2d 52: 90.31%\nSparsity in Conv2d 55: 94.71%\nSparsity in Conv2d 58: 0.41%\nSparsity in Conv2d 61: 96.16%\nSparsity in Conv2d 64: 84.94%\n\n\nFor more information and examples about the pruning criteria, I suggest you to take a look at the corresponding section.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorial.sparsifier.html#remark",
    "href": "tutorial.sparsifier.html#remark",
    "title": "Sparsifier",
    "section": "Remark",
    "text": "Remark\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of 8, this can be done by passing the round_to parameter.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'filter', 'local', large_final)\nsparsifier.sparsify_model(70, round_to=8)\n\n\nsparsifier.print_sparsity()\n\nSparsity in Conv2d 1: 62.50%\nSparsity in Conv2d 7: 62.50%\nSparsity in Conv2d 10: 62.50%\nSparsity in Conv2d 13: 62.50%\nSparsity in Conv2d 16: 62.50%\nSparsity in Conv2d 20: 68.75%\nSparsity in Conv2d 23: 68.75%\nSparsity in Conv2d 26: 68.75%\nSparsity in Conv2d 29: 68.75%\nSparsity in Conv2d 32: 68.75%\nSparsity in Conv2d 36: 68.75%\nSparsity in Conv2d 39: 68.75%\nSparsity in Conv2d 42: 68.75%\nSparsity in Conv2d 45: 68.75%\nSparsity in Conv2d 48: 68.75%\nSparsity in Conv2d 52: 68.75%\nSparsity in Conv2d 55: 68.75%\nSparsity in Conv2d 58: 68.75%\nSparsity in Conv2d 61: 68.75%\nSparsity in Conv2d 64: 68.75%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'filter', 'global', large_final)\nsparsifier.sparsify_model(70, round_to=8)\n\n\nsparsifier.print_sparsity()\n\nSparsity in Conv2d 1: 87.50%\nSparsity in Conv2d 7: 0.00%\nSparsity in Conv2d 10: 0.00%\nSparsity in Conv2d 13: 0.00%\nSparsity in Conv2d 16: 0.00%\nSparsity in Conv2d 20: 93.75%\nSparsity in Conv2d 23: 93.75%\nSparsity in Conv2d 26: 0.00%\nSparsity in Conv2d 29: 93.75%\nSparsity in Conv2d 32: 93.75%\nSparsity in Conv2d 36: 96.88%\nSparsity in Conv2d 39: 96.88%\nSparsity in Conv2d 42: 0.00%\nSparsity in Conv2d 45: 96.88%\nSparsity in Conv2d 48: 93.75%\nSparsity in Conv2d 52: 98.44%\nSparsity in Conv2d 55: 98.44%\nSparsity in Conv2d 58: 0.00%\nSparsity in Conv2d 61: 98.44%\nSparsity in Conv2d 64: 96.88%\n\n\nFor more information about granularities at which you can operate, please check the related page.",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "sparse.sparsify_callback.html",
    "href": "sparse.sparsify_callback.html",
    "title": "Sparsify Callback",
    "section": "",
    "text": "SparsifyCallback\n\n SparsifyCallback (sparsity, granularity, context, criteria, schedule,\n                   lth=False, rewind_epoch=0, reset_end=False,\n                   save_tickets=False, model=None, round_to=None,\n                   layer_type=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;)\n\nSparsify model during training\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`",
    "crumbs": [
      "Get Started",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quick Start",
    "section": "",
    "text": "Embark on a journey to supercharge your neural network models with FasterAI, a PyTorch-based library dedicated exclusively to advanced compression techniques. In today’s fast-paced world, where efficiency and performance are paramount, FasterAI stands out by providing cutting-edge solutions designed to make your neural networks not just lighter, but significantly faster.",
    "crumbs": [
      "Get Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#why-choose-fasterai",
    "href": "quickstart.html#why-choose-fasterai",
    "title": "Quick Start",
    "section": "Why Choose FasterAI?",
    "text": "Why Choose FasterAI?\n\nStreamlined Efficiency: Dive into a suite of compression methodologies, including sparsification, pruning, quantization, and knowledge distillation, each tailored to enhance model efficiency without compromising on accuracy.\nEdge-Ready Models: With FasterAI, prepare your models for the edge, ensuring they run smoothly on devices with limited computational resources, from smartphones to IoT devices.\nCutting-edge Technology: Built on the latest research in data and model compression, FasterAI offers tools that are not just powerful but also easy to integrate into your existing workflows.\nVersatility: From image and video compression to deep learning model optimization, FasterAI is versatile enough to handle a wide range of compression needs, making it suitable for various industries and applications.\nOpen and Accessible: As a community-driven project, FasterAI encourages contributions and feedback, ensuring that the library continues to evolve to meet the needs of its users.",
    "crumbs": [
      "Get Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#getting-started-with-fasterai",
    "href": "quickstart.html#getting-started-with-fasterai",
    "title": "Quick Start",
    "section": "Getting Started with FasterAI",
    "text": "Getting Started with FasterAI\nWhether you’re looking to optimize models for production, research, or hobby projects, FasterAI provides the tools and guidance to achieve your goals. Let’s make your neural networks faster and lighter, together.\n\nHow to use fasterai ?\nFasterAI’s integration with the callback system of fastai represents a significant advancement in how compression techniques can be applied to neural networks, particularly during the training phase. This approach allows for a more seamless and flexible implementation of compression strategies, making it possible to optimize models on-the-fly and potentially achieve better efficiency and performance.\n\n\nUnderstanding Callbacks in fastai\nBefore diving into how FasterAI leverages callbacks, it’s important to understand what callbacks are in the context of the fastai library. Callbacks are a programming pattern that allows users to inject custom behavior into certain stages of the training loop or model lifecycle without altering the core logic of the training process. They can be used for a variety of purposes, such as logging metrics, modifying learning rates, or implementing early stopping.\n\n\nFasterAI’s Use of Callbacks\nFasterAI takes advantage of the callback system in fastai to integrate neural network compression techniques directly into the training process. This integration means that instead of applying compression post-training as a separate step, FasterAI allows for compression techniques like pruning, quantization, and knowledge distillation to be applied dynamically as the model trains. Here’s how it enhances the training process:\n\nDynamic Compression: By using callbacks, FasterAI can dynamically adjust the compression parameters based on the model’s performance during training. For example, it can gradually increase the amount of pruning as the model becomes more stable, leading to a more efficient compression process that minimally impacts performance.\nReal-time Optimization: This approach enables real-time optimization of the model. As the model learns and adapts to the data, FasterAI can apply compression techniques in a way that’s informed by the model’s current state, potentially leading to more effective and efficient compression.\nSeamless Integration: Leveraging fastai’s callback system means that users of FasterAI can integrate compression into their training pipelines with minimal code changes. This seamless integration simplifies the process of applying advanced compression techniques, making it accessible even to those with limited experience in model optimization.\n\n\n\nPractical Implications\nFor practitioners, this means they can train models that are not only high-performing but also optimized for size and speed from the outset. It also opens up new possibilities for experimenting with compression techniques during training, which could lead to novel optimization strategies and more efficient models.\nIn essence, FasterAI’s use of the callback system in fastai democratizes the application of sophisticated neural network compression techniques, making them an integral part of the model development lifecycle rather than an afterthought. This approach aligns with the broader goal of developing AI models that are not just powerful but also efficient and adaptable to various deployment environments.\nTo illustrate the practical application of FasterAI’s integration with the fastai callback system for on-the-fly compression during the training phase, let’s walk through an example. This example will demonstrate how to apply dynamic pruning to a neural network model while it’s being trained, leveraging the callback system for seamless integration.\n\n\nSetting Up Your Environment\nFirst, ensure you have both fastai and FasterAI installed in your Python environment. If you haven’t installed these libraries yet, you can do so using pip:\npip install fastai fasterai\n\n\nImporting Necessary Libraries\nBegin by importing the required libraries from fasterai:\nfrom fasterai.sparse.all import *\n\n\nDefining the Dataloader and Learner\nJust get your favorite Dataloader and Learner as usual with fastai:\ndls = get_dls()\nlearner = get_learner()\n\n\nApplying Dynamic Sparsification with a Callback\nNow, integrate FasterAI’s dynamic sparsification into the training process by adding the SparsifyCallback to your model. This callback will apply sparsify dynamically based on the defined parameters:\nsparsify_callback = SparsifyCallback(sparsity, granularity, context, criteria, schedule)\nlearner.fit(n_epoch, max_lr, cbs=[sparsify_callback])\nIn this example, SparsifyCallback is initialized with different parameters (see the tutorial to better understand those parameters). The fit method trains the model for n_epochs, and the SparsifyCallback is passed through the cbs (callbacks) parameter, enabling dynamic sparsification during the training process.\n\n\nObserving the Effects\nAfter training, you can evaluate the model’s performance and size to observe the effects of dynamic sparsification. You should notice a reduction in the model size with minimal impact on accuracy, showcasing the efficiency of integrating compression techniques during training.\n\n\nConclusion\nThis example demonstrates how FasterAI’s integration with the fastai callback system allows for the application of compression techniques like sparsification directly within the training loop. By leveraging callbacks, you can dynamically optimize your neural network models, making them lighter and faster without a significant compromise on performance. This approach not only simplifies the compression process but also opens up new avenues for creating efficient AI models optimized for various deployment scenarios.",
    "crumbs": [
      "Get Started",
      "Quick Start"
    ]
  },
  {
    "objectID": "prune.pruner.html",
    "href": "prune.pruner.html",
    "title": "Pruner",
    "section": "",
    "text": "source\n\nPruner.prune_model\n\n Pruner.prune_model (sparsity, round_to=None)\n\nLet’s try the Pruner with a VGG16 model\n\nmodel = resnet18(); model\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\nThe Prunercan either remove filters based on local criteria (i.e. each layer will be trimmed of the same % of filters)\n\npruner = Pruner(model, 'local', large_final, layer_type=[nn.Conv2d])\npruner.prune_model(30)\nprint(model)\n\nResNet(\n  (conv1): Conv2d(3, 44, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(44, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(44, 89, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(89, 179, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(89, 179, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(179, 358, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(358, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(179, 358, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(358, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(358, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=358, out_features=1000, bias=True)\n)\n\n\nThe Prunercan also remove filters based on global criteria (i.e. each layer will be trimmed of a different % of filters, but we specify the sparsity of the whole network)\n\nmodel = resnet18()\npruner = Pruner(model, 'global', large_final, layer_type=[nn.Conv2d])\npruner.prune_model(50)\nprint(model)\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(42, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(1, 502, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(502, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 502, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(502, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(502, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(1, 502, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(502, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=502, out_features=1000, bias=True)\n)",
    "crumbs": [
      "Get Started",
      "Prune",
      "Pruner"
    ]
  },
  {
    "objectID": "quantize.quantizer.html",
    "href": "quantize.quantizer.html",
    "title": "Quantizer",
    "section": "",
    "text": "source\n\nQuantizer\n\n Quantizer (activation_observer=&lt;class\n            'torch.ao.quantization.observer.MinMaxObserver'&gt;,\n            weight_observer=&lt;class\n            'torch.ao.quantization.observer.MinMaxObserver'&gt;,\n            activation_qtype=torch.qint8, weight_qtype=torch.quint8,\n            granularity='tensor')\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "Get Started",
      "Quantize",
      "Quantizer"
    ]
  },
  {
    "objectID": "tutorial.knowledge_distillation.html",
    "href": "tutorial.knowledge_distillation.html",
    "title": "KnowledgeDistillation Callback",
    "section": "",
    "text": "We’ll illustrate how to use Knowledge Distillation to distill the knowledge of a Resnet34 (the teacher), to a Resnet18 (the student)\nLet’s us grab some data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nThe first step is then to train the teacher model. We’ll start from a pretrained model, ensuring to get good results on our dataset.\n\nteacher = cnn_learner(dls, resnet34, metrics=accuracy)\nteacher.unfreeze()\nteacher.fit_one_cycle(10, 1e-3)\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.721918\n0.643276\n0.841678\n00:09\n\n\n1\n0.484658\n0.604135\n0.828146\n00:08\n\n\n2\n0.401239\n1.103915\n0.815291\n00:08\n\n\n3\n0.394400\n0.318618\n0.860622\n00:08\n\n\n4\n0.276733\n0.276223\n0.878890\n00:08\n\n\n5\n0.187687\n0.515996\n0.851150\n00:08\n\n\n6\n0.127520\n0.230542\n0.911367\n00:08\n\n\n7\n0.071110\n0.233229\n0.924222\n00:08\n\n\n8\n0.044975\n0.199706\n0.931664\n00:08\n\n\n9\n0.031355\n0.177644\n0.939784\n00:08\n\n\n\n\n\n\nWithout KD\nWe’ll now train a Resnet18 from scratch, and without any help from the teacher model, to get that as a baseline\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nstudent.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.608119\n0.594279\n0.679296\n00:07\n\n\n1\n0.577984\n0.637746\n0.690798\n00:07\n\n\n2\n0.543163\n0.532345\n0.732070\n00:07\n\n\n3\n0.508363\n0.468151\n0.772666\n00:07\n\n\n4\n0.464459\n0.442890\n0.780108\n00:07\n\n\n5\n0.405926\n0.410481\n0.816644\n00:07\n\n\n6\n0.355392\n0.429471\n0.821380\n00:07\n\n\n7\n0.278941\n0.365873\n0.838972\n00:07\n\n\n8\n0.218126\n0.366222\n0.855886\n00:07\n\n\n9\n0.165694\n0.367872\n0.857239\n00:07\n\n\n\n\n\n\n\nWith KD\nAnd now we train the same model, but with the help of the teacher. The chosen loss is a combination of the regular classification loss (Cross-Entropy) and a loss pushing the student to learn from the teacher’s predictions.\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.335700\n1.860445\n0.692828\n00:09\n\n\n1\n2.241398\n1.773348\n0.727334\n00:09\n\n\n2\n2.055018\n1.710084\n0.723951\n00:09\n\n\n3\n1.851421\n1.632465\n0.761840\n00:09\n\n\n4\n1.620585\n1.675239\n0.755751\n00:09\n\n\n5\n1.393245\n1.410955\n0.774019\n00:09\n\n\n6\n1.155736\n1.087842\n0.826116\n00:09\n\n\n7\n0.908853\n0.983743\n0.838972\n00:09\n\n\n8\n0.696537\n0.852848\n0.857916\n00:09\n\n\n9\n0.564625\n0.854901\n0.857239\n00:09\n\n\n\n\n\nWhen helped, the student model performs better !\nThere exist more complicated KD losses, such as the one coming from Paying Attention to Attention, where the student tries to replicate the same attention maps of the teacher at intermediate layers.\nUsing such a loss requires to be able to specify from which layer we want to replicate those attention maps. To do so, we have to specify them from their string name, which can be obtained with the get_model_layers function.\nFor example, we set the loss to be applied after each Residual block of our models:\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, Attention, ['layer1', 'layer2', 'layer3', 'layer4'], ['0.4', '0.5', '0.6', '0.7'], weight=0.9)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.088313\n0.088667\n0.679973\n00:09\n\n\n1\n0.079737\n0.077369\n0.719892\n00:09\n\n\n2\n0.070380\n0.065641\n0.765223\n00:09\n\n\n3\n0.061056\n0.061554\n0.792963\n00:09\n\n\n4\n0.055300\n0.058515\n0.790934\n00:09\n\n\n5\n0.048522\n0.052656\n0.830853\n00:09\n\n\n6\n0.040360\n0.047567\n0.847767\n00:09\n\n\n7\n0.032288\n0.046334\n0.855210\n00:09\n\n\n8\n0.023988\n0.045383\n0.868065\n00:09\n\n\n9\n0.020456\n0.044370\n0.866712\n00:09",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorial.yolov8.html#training",
    "href": "tutorial.yolov8.html#training",
    "title": "YOLOV8",
    "section": "Training",
    "text": "Training\n\nclass Args(argparse.Namespace):\n  model = 'yolov8l.pt'\n  cfg = 'default.yaml'\n  iterative_steps = 15\n  target_prune_rate = 0.15\n  max_map_drop = 0.2\n  sched = Schedule(partial(sched_onecycle,  α=10, β=4))\n\nargs=Args()\nprune(args)\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.731      0.768      0.828      0.659\nSpeed: 0.1ms preprocess, 7.7ms inference, 0.0ms loss, 0.6ms postprocess per image\nResults saved to runs/detect/val59\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train49\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nBefore Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.65869\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/train49/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/train49\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      14.4G     0.8537     0.7447      1.082        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.776      0.741      0.832      0.667\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      12.8G     0.8612     0.7059      1.079        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.859       0.75      0.861      0.697\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      12.7G     0.8249     0.6306      1.054        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.882      0.753      0.862      0.709\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      12.8G     0.7998     0.5746      1.047         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.882      0.799       0.87      0.721\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      13.1G     0.8028     0.5566      1.034         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.88      0.804      0.876      0.728\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      12.8G     0.8042     0.5415      1.047        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.896      0.833      0.901      0.741\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      12.8G     0.7493     0.5095      1.003         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.906      0.827      0.902      0.746\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      12.8G     0.7589     0.5373      1.012        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.91      0.828      0.903      0.749\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      12.8G     0.7234     0.4783     0.9947        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.912      0.832      0.906      0.754\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      12.7G     0.7445     0.4764     0.9944        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.905      0.839      0.905      0.754\n\n10 epochs completed in 0.027 hours.\nOptimizer stripped from runs/detect/train49/weights/last.pt, 175.3MB\nOptimizer stripped from runs/detect/train49/weights/best.pt, 175.3MB\n\nValidating runs/detect/train49/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.904      0.841      0.905      0.755\nSpeed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.3ms postprocess per image\nResults saved to runs/detect/train49\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.917      0.823      0.901      0.754\nSpeed: 0.2ms preprocess, 10.8ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/baseline_val184\n\n\nBefore Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.75438\nConv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n0.27046189978777607\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 43325836 parameters, 74176 gradients, 163.3 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.883      0.849      0.903      0.743\nSpeed: 0.2ms preprocess, 12.4ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_0_pre_val131\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_0_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_0_finetune103\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 1: MACs=81.8125668 G, #Params=43.348966 M, mAP=0.7428735001565969, speed up=1.0111699172357467\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_0_finetune103/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_0_finetune103\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.6G     0.7161     0.4777     0.9953        122        640: 100%|██████████| 8/8 [00:03\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.92      0.841      0.907       0.75\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      13.3G     0.6503     0.4152     0.9541        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.904      0.851       0.91      0.765\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      13.2G     0.6809      0.434     0.9746        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.902      0.854      0.907      0.768\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      13.2G     0.6464     0.4095     0.9681         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.919      0.853      0.913      0.773\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      13.2G     0.6799     0.4357     0.9637         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.916      0.856      0.918      0.779\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      13.2G     0.6787     0.4261     0.9708        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.924      0.851      0.923       0.78\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      13.2G     0.6758     0.4286      0.957         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.91      0.852       0.92      0.785\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      13.2G     0.6818     0.4492     0.9705        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.899      0.856      0.921      0.786\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      13.2G     0.6594      0.429     0.9635        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.894      0.871      0.925      0.794\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      13.2G     0.6705     0.4277     0.9541        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.899       0.87      0.927      0.793\n\n10 epochs completed in 0.038 hours.\nOptimizer stripped from runs/detect/step_0_finetune103/weights/last.pt, 173.9MB\nOptimizer stripped from runs/detect/step_0_finetune103/weights/best.pt, 173.9MB\n\nValidating runs/detect/step_0_finetune103/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 43325836 parameters, 0 gradients, 163.3 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.895       0.87      0.925      0.793\nSpeed: 0.1ms preprocess, 5.1ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_0_finetune103\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 43325836 parameters, 0 gradients, 163.3 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.897      0.863      0.922      0.787\nSpeed: 0.2ms preprocess, 12.4ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_0_post_val75\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.7869655326484724\nAfter post fine-tuning validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.5179586515491672\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 43081939 parameters, 74176 gradients, 162.7 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.913      0.872      0.929      0.788\nSpeed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_1_pre_val66\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_1_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_1_finetune62\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 2: MACs=81.5020432 G, #Params=43.105009 M, mAP=0.7879549975477981, speed up=1.0150224847369225\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_1_finetune62/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_1_finetune62\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.3G     0.5906     0.3832     0.9224        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.903      0.868      0.925      0.797\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      13.3G     0.5313     0.3408     0.9001        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.908       0.87      0.925      0.796\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      13.2G     0.5791     0.3608     0.9246        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.902      0.871      0.927      0.799\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      13.3G     0.5612     0.3602     0.9298         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.914      0.867      0.929      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      13.2G     0.5719     0.3787     0.9132         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.906      0.875       0.93      0.792\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      13.2G     0.5878     0.3844     0.9333        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.935      0.859       0.93      0.796\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      13.3G     0.5939     0.3776     0.9134         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.928       0.86      0.928      0.799\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      13.2G     0.6093     0.3903     0.9311        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.862      0.933      0.802\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      13.3G     0.6003      0.386     0.9318        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.92      0.877      0.935      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      13.2G     0.6393      0.408     0.9322        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.911      0.888      0.937      0.806\n\n10 epochs completed in 0.025 hours.\nOptimizer stripped from runs/detect/step_1_finetune62/weights/last.pt, 173.0MB\nOptimizer stripped from runs/detect/step_1_finetune62/weights/best.pt, 173.0MB\n\nValidating runs/detect/step_1_finetune62/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.911      0.888      0.937      0.806\nSpeed: 0.1ms preprocess, 5.1ms inference, 0.0ms loss, 0.3ms postprocess per image\nResults saved to runs/detect/step_1_finetune62\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.907      0.888      0.937      0.804\nSpeed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_1_post_val48\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.804165683147925\nAfter post fine-tuning validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.9769531739708688\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 42712366 parameters, 74176 gradients, 161.3 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.923      0.871      0.933      0.794\nSpeed: 0.2ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_2_pre_val50\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_2_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_2_finetune48\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 3: MACs=80.7933916 G, #Params=42.735334 M, mAP=0.7940590327289188, speed up=1.0239254072854147\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_2_finetune48/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_2_finetune48\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.4G      0.548     0.3528     0.9023        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.923      0.871      0.935      0.801\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      13.3G     0.4746     0.3015     0.8763        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.93      0.877       0.94      0.806\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      13.2G     0.5379     0.3445     0.9065        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.871      0.942      0.812\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      13.3G     0.5157     0.3339     0.9019         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.93      0.877      0.938      0.811\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      13.3G     0.5169     0.3404     0.8804         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.921      0.883      0.939       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      13.3G     0.5339     0.3559     0.9031        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.929      0.878       0.94      0.811\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      13.3G     0.5597     0.3561     0.8945         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.876      0.941      0.813\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      13.3G     0.5733     0.3857     0.9149        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.936      0.879      0.941      0.818\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      13.3G     0.5769     0.3717     0.9222        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.941      0.882      0.941      0.821\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      13.2G     0.6167     0.3871     0.9151        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.882      0.941      0.821\n\n10 epochs completed in 0.024 hours.\nOptimizer stripped from runs/detect/step_2_finetune48/weights/last.pt, 171.5MB\nOptimizer stripped from runs/detect/step_2_finetune48/weights/best.pt, 171.5MB\n\nValidating runs/detect/step_2_finetune48/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.882      0.942      0.822\nSpeed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_2_finetune48\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.92      0.888      0.943      0.813\nSpeed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_2_post_val42\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8133375576554807\nAfter post fine-tuning validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n1.7924759478681729\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 42094706 parameters, 74176 gradients, 158.8 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.864      0.936      0.804\nSpeed: 0.1ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_3_pre_val36\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_3_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_3_finetune36\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 4: MACs=79.5541908 G, #Params=42.117503 M, mAP=0.8043294271876973, speed up=1.0398749024796818\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_3_finetune36/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_3_finetune36\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.9G     0.5395     0.3534      0.897        122        640: 100%|██████████| 8/8 [00:42\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.875      0.937      0.808\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      13.2G     0.4523     0.2943     0.8601        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.875      0.942      0.816\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      13.2G     0.5011     0.3242      0.885        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.879       0.94      0.816\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      13.2G     0.4896     0.3239      0.881         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.873      0.941      0.818\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      13.2G     0.4877     0.3266     0.8653         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.924      0.883      0.939      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      13.2G     0.5175      0.341     0.8913        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.879      0.943      0.824\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      13.2G     0.5484     0.3518     0.8896         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.882      0.944      0.825\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      13.2G     0.5657     0.3636      0.901        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.947      0.877      0.944      0.827\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      13.2G     0.5557     0.3553      0.908        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.888      0.945      0.827\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      13.2G     0.6072      0.381     0.9066        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.938      0.885      0.945      0.827\n\n10 epochs completed in 0.037 hours.\nOptimizer stripped from runs/detect/step_3_finetune36/weights/last.pt, 169.0MB\nOptimizer stripped from runs/detect/step_3_finetune36/weights/best.pt, 169.0MB\n\nValidating runs/detect/step_3_finetune36/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.947      0.877      0.944      0.827\nSpeed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_3_finetune36\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.875      0.943      0.824\nSpeed: 0.2ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_3_post_val34\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8242263974664863\nAfter post fine-tuning validation\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n3.1368842425083825\nAfter Pruning\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 40919781 parameters, 74176 gradients, 154.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.913      0.876      0.935      0.792\nSpeed: 0.2ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_4_pre_val32\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_4_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_4_finetune32\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 5: MACs=77.3600192 G, #Params=40.942254 M, mAP=0.7920074671210469, speed up=1.0693690003634333\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_4_finetune32/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_4_finetune32\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.8G      0.573     0.3665     0.9011        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.926      0.881       0.94      0.803\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      13.4G     0.4596     0.2974     0.8554        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.879      0.943      0.815\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      13.2G     0.5037     0.3324     0.8775        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.875       0.94      0.815\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      13.3G     0.4863      0.313     0.8774         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.954      0.872      0.943      0.813\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      13.2G     0.4905     0.3254     0.8586         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942       0.87      0.939      0.812\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      13.2G     0.5016     0.3312     0.8863        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.914      0.888      0.939      0.812\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      13.2G     0.5446     0.3534     0.8808         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.937      0.874      0.942      0.818\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      13.2G      0.554     0.3697     0.8957        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.927      0.885      0.942      0.821\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      13.2G     0.5756      0.359     0.9062        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.889      0.945      0.825\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      13.2G     0.6089     0.3807     0.9001        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.884      0.945      0.826\n\n10 epochs completed in 0.035 hours.\nOptimizer stripped from runs/detect/step_4_finetune32/weights/last.pt, 164.3MB\nOptimizer stripped from runs/detect/step_4_finetune32/weights/best.pt, 164.3MB\n\nValidating runs/detect/step_4_finetune32/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.884      0.945      0.827\nSpeed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.3ms postprocess per image\nResults saved to runs/detect/step_4_finetune32\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.929      0.891      0.944       0.82\nSpeed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_4_post_val31\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.820206153122929\nAfter post fine-tuning validation\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n5.101267981852869\nAfter Pruning\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 39455305 parameters, 74176 gradients, 149.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.916      0.864      0.929      0.789\nSpeed: 0.2ms preprocess, 13.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_5_pre_val31\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_5_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_5_finetune31\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 6: MACs=74.8418608 G, #Params=39.477376 M, mAP=0.7891253582912163, speed up=1.1053494062777232\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_5_finetune31/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_5_finetune31\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.5G     0.5773     0.3687     0.8973        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.904      0.881      0.935      0.801\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10        13G     0.4697     0.3058     0.8551        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.906      0.893      0.941      0.807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      12.9G     0.5138     0.3263     0.8829        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.92      0.889      0.942      0.811\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10        13G     0.4938     0.3293     0.8823         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.928      0.883      0.944      0.813\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      12.9G     0.5047     0.3398     0.8637         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934       0.88      0.943      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10        13G     0.5144     0.3483      0.886        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.869       0.94      0.815\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10        13G      0.539     0.3521      0.879         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.943      0.865      0.941      0.814\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10        13G     0.5493     0.3683      0.895        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934      0.875      0.939      0.817\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10        13G      0.581     0.3607     0.9096        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.876       0.94       0.82\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      12.9G     0.6293     0.3874     0.9156        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.938      0.886      0.946      0.822\n\n10 epochs completed in 0.035 hours.\nOptimizer stripped from runs/detect/step_5_finetune31/weights/last.pt, 158.5MB\nOptimizer stripped from runs/detect/step_5_finetune31/weights/best.pt, 158.5MB\n\nValidating runs/detect/step_5_finetune31/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.938      0.886      0.946      0.822\nSpeed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_5_finetune31\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.928      0.891      0.943       0.82\nSpeed: 0.2ms preprocess, 12.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_5_post_val31\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8197008467567249\nAfter post fine-tuning validation\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n7.518590641324997\nAfter Pruning\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 37708749 parameters, 74176 gradients, 143.2 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.904      0.848      0.923       0.76\nSpeed: 0.2ms preprocess, 10.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_6_pre_val28\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_6_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_6_finetune28\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 7: MACs=71.732976 G, #Params=37.730325 M, mAP=0.7604747253578685, speed up=1.1532549046898597\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_6_finetune28/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_6_finetune28\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      13.3G     0.6267     0.3973     0.9214        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.909      0.859      0.932      0.782\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      12.8G     0.5114     0.3263     0.8662        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.874      0.939      0.803\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      12.7G     0.5466     0.3434     0.8876        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.931      0.875      0.939      0.808\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      12.7G     0.5238     0.3378     0.8878         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.874      0.939       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      12.7G     0.5198     0.3522     0.8708         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.947       0.87       0.94      0.808\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      12.8G     0.5276      0.352     0.8876        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.933      0.879      0.939       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      12.8G     0.5516     0.3594     0.8792         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.931      0.882      0.943       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      12.8G     0.5795     0.3736     0.9109        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.933      0.893      0.949      0.817\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      12.8G      0.597     0.3801     0.9136        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932       0.89      0.948      0.815\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      12.7G     0.6406     0.3889     0.9059        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.928      0.899      0.948      0.816\n\n10 epochs completed in 0.034 hours.\nOptimizer stripped from runs/detect/step_6_finetune28/weights/last.pt, 151.5MB\nOptimizer stripped from runs/detect/step_6_finetune28/weights/best.pt, 151.5MB\n\nValidating runs/detect/step_6_finetune28/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.933      0.889      0.948      0.817\nSpeed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_6_finetune28\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.937      0.878      0.946      0.808\nSpeed: 0.1ms preprocess, 10.8ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_6_post_val27\n\n\nAfter fine tuning mAP=0.8082043641470185\nAfter post fine-tuning validation\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n9.935913300797125\nAfter Pruning\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 35995675 parameters, 74176 gradients, 136.7 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.838      0.847      0.905      0.744\nSpeed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_7_pre_val25\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_7_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_7_finetune25\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 8: MACs=68.4860368 G, #Params=36.016747 M, mAP=0.7439133908787243, speed up=1.207930992438447\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_7_finetune25/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_7_finetune25\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10        13G     0.6576     0.4219     0.9433        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.884      0.852      0.921      0.764\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      13.2G     0.5285     0.3538     0.8714        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.921      0.864      0.937      0.782\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      12.4G     0.5672     0.3781     0.8972        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.921       0.87       0.94      0.791\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      12.4G     0.5324     0.3593     0.8898         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.921      0.869      0.937      0.796\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      12.1G     0.5564      0.395     0.8841         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.926      0.888      0.941      0.799\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      12.3G     0.5555     0.3674     0.9059        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.92      0.891      0.942      0.797\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      12.3G     0.5972     0.3946     0.9014         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.909      0.897      0.942      0.797\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      12.3G     0.6033     0.4048     0.9106        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.919      0.892      0.943      0.805\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      12.3G     0.6098     0.3878     0.9253        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.933      0.884      0.944      0.808\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      12.3G     0.6518     0.4124     0.9181        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934      0.887      0.945      0.811\n\n10 epochs completed in 0.036 hours.\nOptimizer stripped from runs/detect/step_7_finetune25/weights/last.pt, 144.6MB\nOptimizer stripped from runs/detect/step_7_finetune25/weights/best.pt, 144.6MB\n\nValidating runs/detect/step_7_finetune25/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934      0.887      0.945       0.81\nSpeed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_7_finetune25\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934      0.877      0.942      0.805\nSpeed: 0.1ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_7_post_val25\n\n\nAfter fine tuning mAP=0.8047311680941978\nAfter post fine-tuning validation\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n11.900297040141613\nAfter Pruning\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 34583399 parameters, 74176 gradients, 131.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.861      0.846      0.915      0.747\nSpeed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_8_pre_val24\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_8_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_8_finetune24\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 9: MACs=65.8289424 G, #Params=34.604045 M, mAP=0.746892685800743, speed up=1.2566874597092115\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_8_finetune24/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_8_finetune24\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      12.7G     0.6527     0.4186     0.9399        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.878       0.86      0.925      0.769\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      12.9G     0.5123     0.3376     0.8642        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.902      0.884      0.932       0.78\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      12.2G     0.5575     0.3672     0.8903        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.918      0.887      0.935      0.784\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      12.2G     0.5313     0.3422     0.8975         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.913      0.897      0.936      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      12.2G      0.543     0.3699      0.874         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.922      0.891      0.939      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      12.2G     0.5544     0.3693        0.9        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.936      0.885      0.938      0.798\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      12.2G     0.5915     0.3854     0.8924         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.883      0.939      0.801\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10        12G     0.6192     0.4081     0.9123        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.882       0.94      0.803\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      12.2G     0.6259     0.4123     0.9284        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.947       0.88      0.941      0.806\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      12.1G     0.6654     0.4213     0.9262        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.886       0.94      0.808\n\n10 epochs completed in 0.037 hours.\nOptimizer stripped from runs/detect/step_8_finetune24/weights/last.pt, 139.0MB\nOptimizer stripped from runs/detect/step_8_finetune24/weights/best.pt, 139.0MB\n\nValidating runs/detect/step_8_finetune24/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.948      0.882       0.94      0.808\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_8_finetune24\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.885       0.94      0.804\nSpeed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_8_post_val24\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8042272329558376\nAfter post fine-tuning validation\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n13.24470533478182\nAfter Pruning\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 33747610 parameters, 74176 gradients, 128.5 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.919      0.872      0.923      0.774\nSpeed: 0.2ms preprocess, 13.1ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_9_pre_val23\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_9_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_9_finetune23\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 10: MACs=64.3900056 G, #Params=33.768007 M, mAP=0.77353892505729, speed up=1.2847709148203583\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_9_finetune23/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_9_finetune23\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      12.6G     0.6022     0.3899     0.9207        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.921      0.881       0.93      0.784\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      12.1G     0.4755     0.3118      0.851        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.941      0.883      0.933      0.794\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10        12G     0.5226     0.3441     0.8847        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.936      0.884      0.936      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      11.8G     0.5197     0.3324     0.8815         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.935      0.883      0.933      0.792\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10        12G     0.5239      0.353     0.8671         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.936      0.882      0.934      0.792\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10        12G     0.5413     0.3589     0.8919        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.93      0.877      0.934      0.802\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      12.1G     0.5753     0.3723     0.8863         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.873      0.933      0.802\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      12.1G     0.6104     0.3991     0.9113        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.95      0.868      0.931        0.8\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      12.1G     0.6059      0.395     0.9182        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.943      0.873      0.932      0.805\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10        12G     0.6558     0.4098     0.9218        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.871      0.933      0.805\n\n10 epochs completed in 0.033 hours.\nOptimizer stripped from runs/detect/step_9_finetune23/weights/last.pt, 135.6MB\nOptimizer stripped from runs/detect/step_9_finetune23/weights/best.pt, 135.6MB\n\nValidating runs/detect/step_9_finetune23/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.871      0.933      0.806\nSpeed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_9_finetune23\n\n\nAfter fine-tuning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.943      0.875      0.932      0.804\nSpeed: 0.2ms preprocess, 14.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_9_post_val23\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8042200149576527\nAfter post fine-tuning validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n14.060228108679125\nAfter Pruning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 33209910 parameters, 74176 gradients, 126.7 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.933      0.855      0.928      0.782\nSpeed: 0.2ms preprocess, 13.6ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_10_pre_val17\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_10_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_10_finetune17\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 11: MACs=63.4942128 G, #Params=33.230145 M, mAP=0.7824563352367453, speed up=1.302896795658832\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_10_finetune17/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_10_finetune17\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      12.3G     0.5909     0.3739      0.911        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.938      0.863      0.931      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      11.9G     0.4459     0.2951     0.8389        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942       0.87      0.932      0.802\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      11.8G     0.5232     0.3395     0.8787        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.946      0.875      0.935      0.802\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      11.8G     0.4976     0.3318      0.877         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.878      0.934      0.795\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      11.8G     0.5079     0.3425     0.8612         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.946      0.875      0.934      0.798\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      11.8G     0.5287     0.3422     0.8902        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.951      0.875      0.937      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      11.9G     0.5654     0.3632     0.8837         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.951      0.879      0.938      0.803\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      11.9G     0.5918     0.3874     0.9027        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.948      0.878      0.937      0.802\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      11.9G     0.6008     0.3761      0.914        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.953      0.876      0.939      0.807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      11.8G     0.6525     0.4039     0.9107        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.951      0.876      0.939      0.806\n\n10 epochs completed in 0.032 hours.\nOptimizer stripped from runs/detect/step_10_finetune17/weights/last.pt, 133.4MB\nOptimizer stripped from runs/detect/step_10_finetune17/weights/best.pt, 133.4MB\n\nValidating runs/detect/step_10_finetune17/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.952      0.876      0.937      0.807\nSpeed: 0.1ms preprocess, 4.3ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_10_finetune17\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.949      0.873      0.938      0.803\nSpeed: 0.2ms preprocess, 14.4ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_10_post_val17\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8030008974391184\nAfter post fine-tuning validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n14.519222631100824\nAfter Pruning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32703049 parameters, 74176 gradients, 124.6 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.926      0.871      0.929      0.785\nSpeed: 0.1ms preprocess, 15.1ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_11_pre_val15\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_11_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_11_finetune15\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 12: MACs=62.4345712 G, #Params=32.723122 M, mAP=0.7849986248769537, speed up=1.3250096030130178\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_11_finetune15/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_11_finetune15\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      12.4G      0.592     0.3808     0.9108        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.912      0.894      0.937      0.794\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      12.5G     0.4274     0.2822     0.8394        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.936       0.88       0.94      0.808\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      11.6G     0.4962     0.3279     0.8649        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934      0.885      0.942      0.807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      11.6G     0.4768     0.3227     0.8737         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.946      0.877      0.938      0.807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      11.5G     0.4901     0.3294     0.8562         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.884      0.939      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      11.5G     0.5087     0.3373     0.8861        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.883      0.939       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      11.5G     0.5481     0.3556     0.8798         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.919       0.89      0.936      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      11.6G     0.5694     0.3718     0.8979        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.937      0.888      0.938      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      11.6G     0.5756     0.3754     0.9038        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.886      0.939      0.806\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      11.5G     0.6536     0.3981     0.9224        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.95      0.883      0.939      0.809\n\n10 epochs completed in 0.030 hours.\nOptimizer stripped from runs/detect/step_11_finetune15/weights/last.pt, 131.4MB\nOptimizer stripped from runs/detect/step_11_finetune15/weights/best.pt, 131.4MB\n\nValidating runs/detect/step_11_finetune15/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.883      0.939      0.809\nSpeed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_11_finetune15\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.885      0.938      0.803\nSpeed: 0.2ms preprocess, 15.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_11_post_val14\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8028105881367777\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n14.766719382862217\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32669140 parameters, 74176 gradients, 124.6 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.883      0.942      0.806\nSpeed: 0.1ms preprocess, 15.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_12_pre_val14\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_12_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_12_finetune13\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 13: MACs=62.4070664 G, #Params=32.689204 M, mAP=0.8058915915724488, speed up=1.325593577332454\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_12_finetune13/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_12_finetune13\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      12.7G      0.499     0.3319     0.8801        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.933      0.887      0.937      0.809\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      11.6G     0.3689     0.2572     0.8209        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.932      0.886      0.938      0.812\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      11.6G     0.4539      0.302     0.8599        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.949      0.876      0.938       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      11.6G     0.4334     0.2969     0.8595         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.94      0.893      0.941      0.808\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      11.6G     0.4529     0.3123     0.8466         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.894      0.941      0.807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      11.6G     0.4631     0.3128     0.8697        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.95      0.887      0.943      0.809\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      11.6G     0.5213     0.3408     0.8692         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.941      0.891      0.939      0.811\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      11.7G      0.539     0.3604     0.8853        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.892       0.94       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      11.7G     0.5515      0.358     0.8976        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.929      0.897       0.94      0.813\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      11.6G     0.6402     0.3891     0.9106        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.941      0.897      0.943      0.816\n\n10 epochs completed in 0.029 hours.\nOptimizer stripped from runs/detect/step_12_finetune13/weights/last.pt, 131.3MB\nOptimizer stripped from runs/detect/step_12_finetune13/weights/best.pt, 131.3MB\n\nValidating runs/detect/step_12_finetune13/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.942      0.897      0.944      0.816\nSpeed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_12_finetune13\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.937      0.892      0.941      0.811\nSpeed: 0.2ms preprocess, 16.7ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_12_post_val13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8111457220640336\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n14.89709551315643\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.939      0.886       0.94      0.805\nSpeed: 0.2ms preprocess, 16.6ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_13_pre_val13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_13_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_13_finetune13\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\nAMP: checks passed ✅\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 14: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8050285863373501, speed up=1.3375568226839933\n\n\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_13_finetune13/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_13_finetune13\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      12.1G     0.5096      0.332     0.8815        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.935      0.891      0.943      0.812\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      11.6G     0.3826     0.2558     0.8262        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.956       0.88      0.949      0.813\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      11.4G     0.4512     0.3126     0.8555        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.948      0.883      0.945      0.814\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      11.5G     0.4423      0.299     0.8562         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.95      0.882      0.944      0.811\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      11.5G     0.4557     0.3122     0.8492         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.935      0.896      0.945      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      11.5G     0.4734     0.3233     0.8728        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.93      0.892      0.941      0.805\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      11.5G     0.5367      0.352     0.8796         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.934      0.886      0.942      0.804\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      11.5G     0.5403     0.3508     0.8848        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.941      0.886      0.943       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      11.5G     0.5416     0.3534      0.889        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.893      0.947      0.814\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      11.5G     0.6465     0.4027     0.9187        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.945      0.891      0.947      0.818\n\n10 epochs completed in 0.030 hours.\nOptimizer stripped from runs/detect/step_13_finetune13/weights/last.pt, 130.3MB\nOptimizer stripped from runs/detect/step_13_finetune13/weights/best.pt, 130.3MB\n\nValidating runs/detect/step_13_finetune13/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.891      0.947      0.819\nSpeed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_13_finetune13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.951      0.887      0.946      0.815\nSpeed: 0.2ms preprocess, 17.3ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_13_post_val13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine tuning mAP=0.8146198835662797\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n14.96493134246744\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.951      0.886      0.945      0.815\nSpeed: 0.2ms preprocess, 17.4ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_14_pre_val13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_14_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_14_finetune13\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 15: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8153698637584581, speed up=1.3375568226839933\n\n\nAMP: checks passed ✅\ntrain: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\nPlotting labels to runs/detect/step_14_finetune13/labels.jpg... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_14_finetune13\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/10      11.4G     0.4922     0.3236     0.8733        122        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.955      0.882      0.946      0.822\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/10      11.4G     0.3523     0.2478     0.8197        112        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.953       0.89      0.943      0.817\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/10      11.4G     0.4283     0.2858     0.8487        116        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929       0.95       0.89      0.941       0.82\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/10      11.4G      0.408     0.2829     0.8446         68        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.953      0.886      0.945      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/10      11.4G     0.4299     0.2959     0.8395         96        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.943      0.902      0.944      0.823\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/10      11.4G     0.4327     0.3007     0.8581        120        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.946      0.889      0.943       0.82\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/10      11.4G     0.4762     0.3178     0.8584         69        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.944      0.892      0.945      0.814\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/10      11.4G     0.5052     0.3337     0.8724        141        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.946      0.893      0.944      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/10      11.4G     0.5167     0.3323     0.8776        104        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.943      0.898      0.944      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/10      11.4G     0.5994     0.3758     0.8965        170        640: 100%|██████████| 8/8 [00:02\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.952      0.893      0.946      0.822\n\n10 epochs completed in 0.017 hours.\nOptimizer stripped from runs/detect/step_14_finetune13/weights/last.pt, 130.3MB\nOptimizer stripped from runs/detect/step_14_finetune13/weights/best.pt, 130.3MB\n\nValidating runs/detect/step_14_finetune13/weights/best.pt...\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.943      0.902      0.944      0.823\nSpeed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\nResults saved to runs/detect/step_14_finetune13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n                   all        128        929      0.947      0.899      0.943      0.819\nSpeed: 0.2ms preprocess, 17.2ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_14_post_val13\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CPU\n\n\nAfter fine tuning mAP=0.8192818206570706\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\nPyTorch: starting from runs/detect/step_14_finetune13/weights/best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (124.2 MB)\n\nONNX: starting export with onnx 1.16.0 opset 17...\nONNX: export success ✅ 2.6s, saved as runs/detect/step_14_finetune13/weights/best.onnx (123.9 MB)\n\nExport complete (3.5s)\nResults saved to /home/HubensN/fasterai/nbs/runs/detect/step_14_finetune13/weights\nPredict:         yolo predict task=detect model=runs/detect/step_14_finetune13/weights/best.onnx imgsz=640 \nValidate:        yolo val task=detect model=runs/detect/step_14_finetune13/weights/best.onnx imgsz=640 data=/home/HubensN/miniconda3/envs/fasterai/lib/python3.9/site-packages/ultralytics/datasets/coco128.yaml \nVisualize:       https://netron.app",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Prune",
      "YOLOV8"
    ]
  },
  {
    "objectID": "tutorial.yolov8.html#post-training-checks",
    "href": "tutorial.yolov8.html#post-training-checks",
    "title": "YOLOV8",
    "section": "Post-Training Checks",
    "text": "Post-Training Checks\n\nmodel = YOLO('/home/HubensN/fasterai/nbs/runs/detect/step_14_finetune4/weights/best.pt')\n\n\nbase_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs); base_macs, base_nparams\n\n(57692198400.0, 30077028)\n\n\n\nresults = model.val(\n                data='coco128.yaml',\n                batch=1,\n                imgsz=640,\n                verbose=False,\n            )\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\nYOLOv8l summary (fused): 285 layers, 30057792 parameters, 0 gradients, 115.1 GFLOPs\nval: Scanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/tra\n                 Class     Images  Instances      Box(P          R  \n                   all        128        929      0.917      0.907      0.945      0.809\nSpeed: 0.2ms preprocess, 24.4ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/val35\n\n\n\nresults\n\nultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\nbox: ultralytics.yolo.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.yolo.utils.metrics.ConfusionMatrix object&gt;\nfitness: 0.8221835652536718\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.75668,     0.47387,      0.3594,     0.91994,     0.94661,     0.90211,     0.94289,     0.68531,     0.68927,     0.34436,     0.80851,      0.8955,     0.80851,     0.86091,     0.84563,     0.96863,     0.89911,      0.9501,     0.80851,     0.80851,     0.88959,       0.995,       0.995,     0.93382,\n            0.8273,     0.84511,      0.6686,     0.77723,     0.80558,      0.8234,      0.8955,     0.68357,     0.40784,     0.61583,     0.67229,     0.39977,     0.87814,     0.80851,     0.60498,     0.56705,       0.726,     0.76821,     0.76074,     0.56956,     0.72918,     0.79545,       0.995,     0.80851,\n             0.995,      0.8713,     0.73808,      0.7727,       0.995,     0.94354,     0.96781,      0.9387,     0.81318,     0.93981,     0.90203,       0.995,     0.80108,     0.90138,       0.995,      0.9641,     0.55702,     0.69798,     0.80851,     0.74254,     0.90157,     0.90353,     0.80851,     0.80032,\n           0.92735,     0.62032,     0.86949,     0.92389,       0.995,      0.9143,     0.80851,     0.94696])\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9173606393289034, 'metrics/recall(B)': 0.9071337022261394, 'metrics/mAP50(B)': 0.9452653330065343, 'metrics/mAP50-95(B)': 0.8085078132811314, 'fitness': 0.8221835652536718}\nsave_dir: Path('runs/detect/val35')\nspeed: {'preprocess': 0.17499923706054688, 'inference': 24.442605674266815, 'loss': 0.0046156346797943115, 'postprocess': 0.376259908080101}\n\n\n\nmodel.export(format = 'onnx', half = True)\n\nUltralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CPU\nWARNING ⚠️ half=True only compatible with GPU export, i.e. use device=0\nYOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n\nPyTorch: starting from yolov8l.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (83.7 MB)\n\nONNX: starting export with onnx 1.16.0 opset 17...\nONNX: export success ✅ 2.8s, saved as yolov8l.onnx (166.8 MB)\n\nExport complete (4.0s)\nResults saved to /home/HubensN/fasterai/nbs\nPredict:         yolo predict task=detect model=yolov8l.onnx imgsz=640 \nValidate:        yolo val task=detect model=yolov8l.onnx imgsz=640 data=coco.yaml \nVisualize:       https://netron.app\n\n\n'yolov8l.onnx'",
    "crumbs": [
      "Get Started",
      "Tutorials",
      "Prune",
      "YOLOV8"
    ]
  }
]